{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c9qJ3qjeAYlk",
        "outputId": "9acd5a21-a5ad-4361-dc86-55c57f7b30f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libinchi1 libmaeparser1 libopenbabel7\n",
            "The following NEW packages will be installed:\n",
            "  libinchi1 libmaeparser1 libopenbabel7 openbabel\n",
            "0 upgraded, 4 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 3,903 kB of archives.\n",
            "After this operation, 16.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libinchi1 amd64 1.03+dfsg-4 [455 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmaeparser1 amd64 1.2.4-1build1 [88.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenbabel7 amd64 3.1.1+dfsg-6ubuntu5 [3,231 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 openbabel amd64 3.1.1+dfsg-6ubuntu5 [128 kB]\n",
            "Fetched 3,903 kB in 3s (1,481 kB/s)\n",
            "Selecting previously unselected package libinchi1.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../libinchi1_1.03+dfsg-4_amd64.deb ...\n",
            "Unpacking libinchi1 (1.03+dfsg-4) ...\n",
            "Selecting previously unselected package libmaeparser1:amd64.\n",
            "Preparing to unpack .../libmaeparser1_1.2.4-1build1_amd64.deb ...\n",
            "Unpacking libmaeparser1:amd64 (1.2.4-1build1) ...\n",
            "Selecting previously unselected package libopenbabel7.\n",
            "Preparing to unpack .../libopenbabel7_3.1.1+dfsg-6ubuntu5_amd64.deb ...\n",
            "Unpacking libopenbabel7 (3.1.1+dfsg-6ubuntu5) ...\n",
            "Selecting previously unselected package openbabel.\n",
            "Preparing to unpack .../openbabel_3.1.1+dfsg-6ubuntu5_amd64.deb ...\n",
            "Unpacking openbabel (3.1.1+dfsg-6ubuntu5) ...\n",
            "Setting up libmaeparser1:amd64 (1.2.4-1build1) ...\n",
            "Setting up libinchi1 (1.03+dfsg-4) ...\n",
            "Setting up libopenbabel7 (3.1.1+dfsg-6ubuntu5) ...\n",
            "Setting up openbabel (3.1.1+dfsg-6ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "--2024-03-26 15:00:39--  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.16.246.55, 104.16.245.55, 2606:4700::6810:f637, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.16.246.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh [following]\n",
            "--2024-03-26 15:00:39--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 144041912 (137M) [application/octet-stream]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-latest-L 100%[===================>] 137.37M  86.6MB/s    in 1.6s    \n",
            "\n",
            "2024-03-26 15:00:41 (86.6 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [144041912/144041912]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "                                                                                \n",
            "Installing base environment...\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Channels:\n",
            " - conda-forge\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.10\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge\n",
            "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
            "    archspec-0.2.3             |     pyhd8ed1ab_0          48 KB  conda-forge\n",
            "    boltons-23.1.1             |     pyhd8ed1ab_0         297 KB  conda-forge\n",
            "    brotli-python-1.1.0        |  py310hc6cd4ac_1         341 KB  conda-forge\n",
            "    ca-certificates-2024.2.2   |       hbcca054_0         152 KB  conda-forge\n",
            "    certifi-2024.2.2           |     pyhd8ed1ab_0         157 KB  conda-forge\n",
            "    cffi-1.16.0                |  py310h2fee648_0         236 KB  conda-forge\n",
            "    colorama-0.4.6             |     pyhd8ed1ab_0          25 KB  conda-forge\n",
            "    conda-24.3.0               |  py310hff52083_0         920 KB  conda-forge\n",
            "    conda-content-trust-0.2.0  |     pyhd8ed1ab_0          57 KB  conda-forge\n",
            "    conda-package-handling-2.2.0|     pyh38be061_0         249 KB  conda-forge\n",
            "    conda-package-streaming-0.9.0|     pyhd8ed1ab_0          19 KB  conda-forge\n",
            "    cryptography-42.0.5        |  py310h75e40e8_0         1.9 MB  conda-forge\n",
            "    distro-1.9.0               |     pyhd8ed1ab_0          41 KB  conda-forge\n",
            "    idna-3.6                   |     pyhd8ed1ab_0          49 KB  conda-forge\n",
            "    libgcc-ng-13.2.0           |       h807b86a_5         752 KB  conda-forge\n",
            "    libgomp-13.2.0             |       h807b86a_5         410 KB  conda-forge\n",
            "    libmambapy-1.5.3           |  py310h2dafd23_0         314 KB\n",
            "    libnsl-2.0.1               |       hd590300_0          33 KB  conda-forge\n",
            "    libsqlite-3.45.2           |       h2797004_0         837 KB  conda-forge\n",
            "    libstdcxx-ng-13.2.0        |       h7e041cc_5         3.7 MB  conda-forge\n",
            "    libuuid-2.38.1             |       h0b41bf4_0          33 KB  conda-forge\n",
            "    libxcrypt-4.4.36           |       hd590300_1          98 KB  conda-forge\n",
            "    libzlib-1.2.13             |       hd590300_5          60 KB  conda-forge\n",
            "    menuinst-2.0.2             |  py310hff52083_0         132 KB  conda-forge\n",
            "    ncurses-6.4.20240210       |       h59595ed_0         875 KB  conda-forge\n",
            "    openssl-3.2.1              |       hd590300_1         2.7 MB  conda-forge\n",
            "    packaging-24.0             |     pyhd8ed1ab_0          49 KB  conda-forge\n",
            "    pip-24.0                   |     pyhd8ed1ab_0         1.3 MB  conda-forge\n",
            "    platformdirs-4.2.0         |     pyhd8ed1ab_0          20 KB  conda-forge\n",
            "    pluggy-1.4.0               |     pyhd8ed1ab_0          23 KB  conda-forge\n",
            "    pycosat-0.6.6              |  py310h2372a71_0          85 KB  conda-forge\n",
            "    pysocks-1.7.1              |     pyha2e5f31_6          19 KB  conda-forge\n",
            "    python-3.10.14             |hd12c33a_0_cpython        24.3 MB  conda-forge\n",
            "    python_abi-3.10            |          4_cp310           6 KB  conda-forge\n",
            "    requests-2.31.0            |     pyhd8ed1ab_0          55 KB  conda-forge\n",
            "    ruamel.yaml-0.18.6         |  py310h2372a71_0         199 KB  conda-forge\n",
            "    ruamel.yaml.clib-0.2.8     |  py310h2372a71_0         133 KB  conda-forge\n",
            "    setuptools-69.2.0          |     pyhd8ed1ab_0         460 KB  conda-forge\n",
            "    tk-8.6.13                  |noxft_h4845f30_101         3.2 MB  conda-forge\n",
            "    tqdm-4.66.2                |     pyhd8ed1ab_0          87 KB  conda-forge\n",
            "    truststore-0.8.0           |     pyhd8ed1ab_0          20 KB  conda-forge\n",
            "    urllib3-2.2.1              |     pyhd8ed1ab_0          92 KB  conda-forge\n",
            "    wheel-0.43.0               |     pyhd8ed1ab_0          57 KB  conda-forge\n",
            "    zlib-1.2.13                |       hd590300_5          91 KB  conda-forge\n",
            "    zstandard-0.22.0           |  py310h1275a96_0         395 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        44.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  colorama           conda-forge/noarch::colorama-0.4.6-pyhd8ed1ab_0 \n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hd590300_0 \n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.45.2-h2797004_0 \n",
            "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.13-hd590300_5 \n",
            "  python_abi         conda-forge/linux-64::python_abi-3.10-4_cp310 \n",
            "  ruamel.yaml.clib   conda-forge/linux-64::ruamel.yaml.clib-0.2.8-py310h2372a71_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  archspec           pkgs/main::archspec-0.2.1-pyhd3eb1b0_0 --> conda-forge::archspec-0.2.3-pyhd8ed1ab_0 \n",
            "  boltons            pkgs/main/linux-64::boltons-23.0.0-py~ --> conda-forge/noarch::boltons-23.1.1-pyhd8ed1ab_0 \n",
            "  brotli-python      pkgs/main::brotli-python-1.0.9-py312h~ --> conda-forge::brotli-python-1.1.0-py310hc6cd4ac_1 \n",
            "  ca-certificates    pkgs/main::ca-certificates-2023.12.12~ --> conda-forge::ca-certificates-2024.2.2-hbcca054_0 \n",
            "  conda              pkgs/main::conda-24.1.2-py312h06a4308~ --> conda-forge::conda-24.3.0-py310hff52083_0 \n",
            "  cryptography       pkgs/main::cryptography-41.0.7-py312h~ --> conda-forge::cryptography-42.0.5-py310h75e40e8_0 \n",
            "  distro             pkgs/main/linux-64::distro-1.8.0-py31~ --> conda-forge/noarch::distro-1.9.0-pyhd8ed1ab_0 \n",
            "  idna               pkgs/main/linux-64::idna-3.4-py312h06~ --> conda-forge/noarch::idna-3.6-pyhd8ed1ab_0 \n",
            "  libgcc-ng          pkgs/main::libgcc-ng-11.2.0-h1234567_1 --> conda-forge::libgcc-ng-13.2.0-h807b86a_5 \n",
            "  libgomp              pkgs/main::libgomp-11.2.0-h1234567_1 --> conda-forge::libgomp-13.2.0-h807b86a_5 \n",
            "  libstdcxx-ng       pkgs/main::libstdcxx-ng-11.2.0-h12345~ --> conda-forge::libstdcxx-ng-13.2.0-h7e041cc_5 \n",
            "  libuuid              pkgs/main::libuuid-1.41.5-h5eee18b_0 --> conda-forge::libuuid-2.38.1-h0b41bf4_0 \n",
            "  ncurses                 pkgs/main::ncurses-6.4-h6a678d5_0 --> conda-forge::ncurses-6.4.20240210-h59595ed_0 \n",
            "  openssl              pkgs/main::openssl-3.0.13-h7f8727e_0 --> conda-forge::openssl-3.2.1-hd590300_1 \n",
            "  packaging          pkgs/main/linux-64::packaging-23.1-py~ --> conda-forge/noarch::packaging-24.0-pyhd8ed1ab_0 \n",
            "  pip                pkgs/main/linux-64::pip-23.3.1-py312h~ --> conda-forge/noarch::pip-24.0-pyhd8ed1ab_0 \n",
            "  platformdirs       pkgs/main/linux-64::platformdirs-3.10~ --> conda-forge/noarch::platformdirs-4.2.0-pyhd8ed1ab_0 \n",
            "  pluggy             pkgs/main/linux-64::pluggy-1.0.0-py31~ --> conda-forge/noarch::pluggy-1.4.0-pyhd8ed1ab_0 \n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py3~ --> conda-forge/noarch::pysocks-1.7.1-pyha2e5f31_6 \n",
            "  ruamel.yaml        pkgs/main::ruamel.yaml-0.17.21-py312h~ --> conda-forge::ruamel.yaml-0.18.6-py310h2372a71_0 \n",
            "  setuptools         pkgs/main/linux-64::setuptools-68.2.2~ --> conda-forge/noarch::setuptools-69.2.0-pyhd8ed1ab_0 \n",
            "  tk                        pkgs/main::tk-8.6.12-h1ccaba5_0 --> conda-forge::tk-8.6.13-noxft_h4845f30_101 \n",
            "  tqdm               pkgs/main/linux-64::tqdm-4.65.0-py312~ --> conda-forge/noarch::tqdm-4.66.2-pyhd8ed1ab_0 \n",
            "  urllib3            pkgs/main/linux-64::urllib3-2.1.0-py3~ --> conda-forge/noarch::urllib3-2.2.1-pyhd8ed1ab_0 \n",
            "  wheel              pkgs/main/linux-64::wheel-0.41.2-py31~ --> conda-forge/noarch::wheel-0.43.0-pyhd8ed1ab_0 \n",
            "  zlib                    pkgs/main::zlib-1.2.13-h5eee18b_0 --> conda-forge::zlib-1.2.13-hd590300_5 \n",
            "  zstandard          pkgs/main::zstandard-0.19.0-py312h5ee~ --> conda-forge::zstandard-0.22.0-py310h1275a96_0 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  _libgcc_mutex           pkgs/main::_libgcc_mutex-0.1-main --> conda-forge::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex          pkgs/main::_openmp_mutex-5.1-1_gnu --> conda-forge::_openmp_mutex-4.5-2_gnu \n",
            "  certifi            pkgs/main/linux-64::certifi-2024.2.2-~ --> conda-forge/noarch::certifi-2024.2.2-pyhd8ed1ab_0 \n",
            "  cffi               pkgs/main::cffi-1.16.0-py312h5eee18b_0 --> conda-forge::cffi-1.16.0-py310h2fee648_0 \n",
            "  conda-content-tru~ pkgs/main/linux-64::conda-content-tru~ --> conda-forge/noarch::conda-content-trust-0.2.0-pyhd8ed1ab_0 \n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-han~ --> conda-forge/noarch::conda-package-handling-2.2.0-pyh38be061_0 \n",
            "  conda-package-str~ pkgs/main/linux-64::conda-package-str~ --> conda-forge/noarch::conda-package-streaming-0.9.0-pyhd8ed1ab_0 \n",
            "  menuinst           pkgs/main::menuinst-2.0.2-py312h06a43~ --> conda-forge::menuinst-2.0.2-py310hff52083_0 \n",
            "  pycosat            pkgs/main::pycosat-0.6.6-py312h5eee18~ --> conda-forge::pycosat-0.6.6-py310h2372a71_0 \n",
            "  python                pkgs/main::python-3.12.1-h996f2a0_0 --> conda-forge::python-3.10.14-hd12c33a_0_cpython \n",
            "  requests           pkgs/main/linux-64::requests-2.31.0-p~ --> conda-forge/noarch::requests-2.31.0-pyhd8ed1ab_0 \n",
            "  truststore         pkgs/main/linux-64::truststore-0.8.0-~ --> conda-forge/noarch::truststore-0.8.0-pyhd8ed1ab_0 \n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  libmambapy                          1.5.3-py312h2dafd23_0 --> 1.5.3-py310h2dafd23_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Channels:\n",
            " - conda-forge\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - dockstring\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.74.0               |  py310h7c3ba0c_5         375 KB  conda-forge\n",
            "    boost-cpp-1.74.0           |       h75c5d50_8        16.2 MB  conda-forge\n",
            "    brotli-1.1.0               |       hd590300_1          19 KB  conda-forge\n",
            "    brotli-bin-1.1.0           |       hd590300_1          19 KB  conda-forge\n",
            "    cairo-1.16.0               |    ha61ee94_1014         1.5 MB  conda-forge\n",
            "    chardet-5.2.0              |  py310hff52083_1         241 KB  conda-forge\n",
            "    contourpy-1.2.0            |  py310hd41b1e2_0         233 KB  conda-forge\n",
            "    cycler-0.12.1              |     pyhd8ed1ab_0          13 KB  conda-forge\n",
            "    dockstring-0.3.2           |     pyhd8ed1ab_0         3.1 MB  conda-forge\n",
            "    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n",
            "    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n",
            "    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n",
            "    font-ttf-ubuntu-0.83       |       h77eed37_1         1.5 MB  conda-forge\n",
            "    fontconfig-2.14.2          |       h14ed4e7_0         266 KB  conda-forge\n",
            "    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n",
            "    fonts-conda-forge-1        |                0           4 KB  conda-forge\n",
            "    fonttools-4.50.0           |  py310h2372a71_0         2.2 MB  conda-forge\n",
            "    freetype-2.12.1            |       h267a509_2         620 KB  conda-forge\n",
            "    freetype-py-2.3.0          |     pyhd8ed1ab_0          58 KB  conda-forge\n",
            "    gettext-0.21.1             |       h27087fc_0         4.1 MB  conda-forge\n",
            "    greenlet-3.0.3             |  py310hc6cd4ac_0         206 KB  conda-forge\n",
            "    icu-70.1                   |       h27087fc_0        13.5 MB  conda-forge\n",
            "    kiwisolver-1.4.5           |  py310hd41b1e2_1          71 KB  conda-forge\n",
            "    lcms2-2.15                 |       haa2dc70_1         236 KB  conda-forge\n",
            "    lerc-4.0.0                 |       h27087fc_0         275 KB  conda-forge\n",
            "    libblas-3.9.0              |21_linux64_openblas          14 KB  conda-forge\n",
            "    libbrotlicommon-1.1.0      |       hd590300_1          68 KB  conda-forge\n",
            "    libbrotlidec-1.1.0         |       hd590300_1          32 KB  conda-forge\n",
            "    libbrotlienc-1.1.0         |       hd590300_1         276 KB  conda-forge\n",
            "    libcblas-3.9.0             |21_linux64_openblas          14 KB  conda-forge\n",
            "    libdeflate-1.18            |       h0b41bf4_0          64 KB  conda-forge\n",
            "    libgfortran-ng-13.2.0      |       h69a702a_5          23 KB  conda-forge\n",
            "    libgfortran5-13.2.0        |       ha4646dd_5         1.4 MB  conda-forge\n",
            "    libglib-2.78.4             |       h783c2da_0         2.6 MB  conda-forge\n",
            "    libiconv-1.17              |       hd590300_2         689 KB  conda-forge\n",
            "    libjpeg-turbo-2.1.5.1      |       hd590300_1         485 KB  conda-forge\n",
            "    liblapack-3.9.0            |21_linux64_openblas          14 KB  conda-forge\n",
            "    libopenblas-0.3.26         |pthreads_h413a1c8_0         5.3 MB  conda-forge\n",
            "    libpng-1.6.43              |       h2797004_0         281 KB  conda-forge\n",
            "    libtiff-4.5.1              |       h8b53f26_1         407 KB  conda-forge\n",
            "    libwebp-base-1.3.2         |       hd590300_0         392 KB  conda-forge\n",
            "    libxcb-1.13                |    h7f98852_1004         391 KB  conda-forge\n",
            "    libxml2-2.10.3             |       hca2bb57_4         697 KB  conda-forge\n",
            "    matplotlib-base-3.8.3      |  py310h62c0568_0         6.7 MB  conda-forge\n",
            "    munkres-1.1.4              |     pyh9f0ad1d_0          12 KB  conda-forge\n",
            "    numpy-1.26.4               |  py310hb13e2d6_0         6.7 MB  conda-forge\n",
            "    openbabel-3.1.1            |  py310heaf86c6_5         5.6 MB  conda-forge\n",
            "    openjpeg-2.5.0             |       hfec8fc6_2         344 KB  conda-forge\n",
            "    pandas-2.2.1               |  py310hcc13569_0        12.4 MB  conda-forge\n",
            "    pillow-9.5.0               |  py310h065c6d2_0        44.3 MB  conda-forge\n",
            "    pixman-0.43.2              |       h59595ed_0         378 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h36c2ea0_1001           5 KB  conda-forge\n",
            "    pycairo-1.25.0             |  py310hda9f760_1         112 KB  conda-forge\n",
            "    pyparsing-3.1.2            |     pyhd8ed1ab_0          87 KB  conda-forge\n",
            "    python-dateutil-2.9.0      |     pyhd8ed1ab_0         218 KB  conda-forge\n",
            "    python-tzdata-2024.1       |     pyhd8ed1ab_0         141 KB  conda-forge\n",
            "    pytz-2024.1                |     pyhd8ed1ab_0         184 KB  conda-forge\n",
            "    rdkit-2022.03.5            |  py310h1c297d8_0        39.7 MB  conda-forge\n",
            "    reportlab-4.1.0            |  py310h2372a71_0         2.2 MB  conda-forge\n",
            "    rlpycairo-0.2.0            |     pyhd8ed1ab_0          15 KB  conda-forge\n",
            "    six-1.16.0                 |     pyh6c4a22f_0          14 KB  conda-forge\n",
            "    sqlalchemy-2.0.29          |  py310h2372a71_0         2.7 MB  conda-forge\n",
            "    typing-extensions-4.10.0   |       hd8ed1ab_0          10 KB  conda-forge\n",
            "    typing_extensions-4.10.0   |     pyha770c72_0          36 KB  conda-forge\n",
            "    unicodedata2-15.1.0        |  py310h2372a71_0         365 KB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h7f98852_1002          27 KB  conda-forge\n",
            "    xorg-libice-1.1.1          |       hd590300_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.4           |       h7391055_0          27 KB  conda-forge\n",
            "    xorg-libx11-1.8.4          |       h0b41bf4_0         810 KB  conda-forge\n",
            "    xorg-libxau-1.0.11         |       hd590300_0          14 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h7f98852_0          19 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h0b41bf4_2          49 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h7f98852_1003          32 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h7f98852_1002           9 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h0b41bf4_1003          30 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h7f98852_1007          73 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       182.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.74.0-py310h7c3ba0c_5 \n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.74.0-h75c5d50_8 \n",
            "  brotli             conda-forge/linux-64::brotli-1.1.0-hd590300_1 \n",
            "  brotli-bin         conda-forge/linux-64::brotli-bin-1.1.0-hd590300_1 \n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-ha61ee94_1014 \n",
            "  chardet            conda-forge/linux-64::chardet-5.2.0-py310hff52083_1 \n",
            "  contourpy          conda-forge/linux-64::contourpy-1.2.0-py310hd41b1e2_0 \n",
            "  cycler             conda-forge/noarch::cycler-0.12.1-pyhd8ed1ab_0 \n",
            "  dockstring         conda-forge/noarch::dockstring-0.3.2-pyhd8ed1ab_0 \n",
            "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n",
            "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n",
            "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n",
            "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-h77eed37_1 \n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0 \n",
            "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n",
            "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n",
            "  fonttools          conda-forge/linux-64::fonttools-4.50.0-py310h2372a71_0 \n",
            "  freetype           conda-forge/linux-64::freetype-2.12.1-h267a509_2 \n",
            "  freetype-py        conda-forge/noarch::freetype-py-2.3.0-pyhd8ed1ab_0 \n",
            "  gettext            conda-forge/linux-64::gettext-0.21.1-h27087fc_0 \n",
            "  greenlet           conda-forge/linux-64::greenlet-3.0.3-py310hc6cd4ac_0 \n",
            "  kiwisolver         conda-forge/linux-64::kiwisolver-1.4.5-py310hd41b1e2_1 \n",
            "  lcms2              conda-forge/linux-64::lcms2-2.15-haa2dc70_1 \n",
            "  lerc               conda-forge/linux-64::lerc-4.0.0-h27087fc_0 \n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-21_linux64_openblas \n",
            "  libbrotlicommon    conda-forge/linux-64::libbrotlicommon-1.1.0-hd590300_1 \n",
            "  libbrotlidec       conda-forge/linux-64::libbrotlidec-1.1.0-hd590300_1 \n",
            "  libbrotlienc       conda-forge/linux-64::libbrotlienc-1.1.0-hd590300_1 \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-21_linux64_openblas \n",
            "  libdeflate         conda-forge/linux-64::libdeflate-1.18-h0b41bf4_0 \n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-13.2.0-h69a702a_5 \n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-13.2.0-ha4646dd_5 \n",
            "  libglib            conda-forge/linux-64::libglib-2.78.4-h783c2da_0 \n",
            "  libiconv           conda-forge/linux-64::libiconv-1.17-hd590300_2 \n",
            "  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-2.1.5.1-hd590300_1 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-21_linux64_openblas \n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.26-pthreads_h413a1c8_0 \n",
            "  libpng             conda-forge/linux-64::libpng-1.6.43-h2797004_0 \n",
            "  libtiff            conda-forge/linux-64::libtiff-4.5.1-h8b53f26_1 \n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.3.2-hd590300_0 \n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h7f98852_1004 \n",
            "  matplotlib-base    conda-forge/linux-64::matplotlib-base-3.8.3-py310h62c0568_0 \n",
            "  munkres            conda-forge/noarch::munkres-1.1.4-pyh9f0ad1d_0 \n",
            "  numpy              conda-forge/linux-64::numpy-1.26.4-py310hb13e2d6_0 \n",
            "  openbabel          conda-forge/linux-64::openbabel-3.1.1-py310heaf86c6_5 \n",
            "  openjpeg           conda-forge/linux-64::openjpeg-2.5.0-hfec8fc6_2 \n",
            "  pandas             conda-forge/linux-64::pandas-2.2.1-py310hcc13569_0 \n",
            "  pillow             conda-forge/linux-64::pillow-9.5.0-py310h065c6d2_0 \n",
            "  pixman             conda-forge/linux-64::pixman-0.43.2-h59595ed_0 \n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001 \n",
            "  pycairo            conda-forge/linux-64::pycairo-1.25.0-py310hda9f760_1 \n",
            "  pyparsing          conda-forge/noarch::pyparsing-3.1.2-pyhd8ed1ab_0 \n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.9.0-pyhd8ed1ab_0 \n",
            "  python-tzdata      conda-forge/noarch::python-tzdata-2024.1-pyhd8ed1ab_0 \n",
            "  pytz               conda-forge/noarch::pytz-2024.1-pyhd8ed1ab_0 \n",
            "  rdkit              conda-forge/linux-64::rdkit-2022.03.5-py310h1c297d8_0 \n",
            "  reportlab          conda-forge/linux-64::reportlab-4.1.0-py310h2372a71_0 \n",
            "  rlpycairo          conda-forge/noarch::rlpycairo-0.2.0-pyhd8ed1ab_0 \n",
            "  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0 \n",
            "  sqlalchemy         conda-forge/linux-64::sqlalchemy-2.0.29-py310h2372a71_0 \n",
            "  typing-extensions  conda-forge/noarch::typing-extensions-4.10.0-hd8ed1ab_0 \n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.10.0-pyha770c72_0 \n",
            "  unicodedata2       conda-forge/linux-64::unicodedata2-15.1.0-py310h2372a71_0 \n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h7f98852_1002 \n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.1-hd590300_0 \n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.4-h7391055_0 \n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.4-h0b41bf4_0 \n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.11-hd590300_0 \n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0 \n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2 \n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h7f98852_1003 \n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h7f98852_1002 \n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003 \n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  icu                        pkgs/main::icu-73.1-h6a678d5_0 --> conda-forge::icu-70.1-h27087fc_0 \n",
            "  libxml2              pkgs/main::libxml2-2.10.4-hf1b16e4_1 --> conda-forge::libxml2-2.10.3-hca2bb57_4 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ]
        }
      ],
      "source": [
        "!apt install openbabel\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -q -y -c conda-forge python=3.10\n",
        "!conda install -q -y -c conda-forge dockstring\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.10/site-packages/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmbYYFIjC5i2"
      },
      "outputs": [],
      "source": [
        "from dockstring import load_target\n",
        "import rdkit.Chem as Chem\n",
        "\n",
        "smiles = 'CC1=C(C(=O)N2CCCCC2=N1)CCN3CCC(CC3)C4=NOC5=C4C=CC(=C5)F'\n",
        "Chem.MolFromSmiles(smiles)\n",
        "target = load_target('DRD2')\n",
        "score, aux = target.dock(smiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HXso-PnH151",
        "outputId": "2658728f-d9a1-4690-b8ab-ab5fa44adbef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-11.9\n"
          ]
        }
      ],
      "source": [
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZzOLh2vVUmc",
        "outputId": "6ed9bbcb-22e9-4d0b-ddfa-e3cb51ddcaf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-26 01:31:52--  https://figshare.com/ndownloader/files/35948165\n",
            "Resolving figshare.com (figshare.com)... 54.76.53.103, 54.73.41.93, 2a05:d018:1f4:d003:8a09:a9ea:f161:d4fe, ...\n",
            "Connecting to figshare.com (figshare.com)|54.76.53.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/35948165/FGFR1.sdf.xz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240326/eu-west-1/s3/aws4_request&X-Amz-Date=20240326T013153Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=43b6284e9c5bf6eebdb13331ed8137182860ad26672ad05dcdd21a1ae9888cbe [following]\n",
            "--2024-03-26 01:31:53--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/35948165/FGFR1.sdf.xz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240326/eu-west-1/s3/aws4_request&X-Amz-Date=20240326T013153Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=43b6284e9c5bf6eebdb13331ed8137182860ad26672ad05dcdd21a1ae9888cbe\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.44.80, 52.218.116.168, 52.218.36.82, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.44.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63427228 (60M) [application/x-xz]\n",
            "Saving to: ‘data/FGFR1.sdf.xz’\n",
            "\n",
            "data/FGFR1.sdf.xz   100%[===================>]  60.49M  16.5MB/s    in 4.0s    \n",
            "\n",
            "2024-03-26 01:31:58 (15.3 MB/s) - ‘data/FGFR1.sdf.xz’ saved [63427228/63427228]\n",
            "\n",
            "unxz: data/FGFR1.sdf: File exists\n",
            "--2024-03-26 01:31:58--  https://figshare.com/ndownloader/files/35948150\n",
            "Resolving figshare.com (figshare.com)... 54.76.53.103, 54.73.41.93, 2a05:d018:1f4:d003:8a09:a9ea:f161:d4fe, ...\n",
            "Connecting to figshare.com (figshare.com)|54.76.53.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/35948150/EGFR.sdf.xz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240326/eu-west-1/s3/aws4_request&X-Amz-Date=20240326T013158Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=bdc4bfec8c1d208b9030b9ddbad80393ce461404d0828d6e8c8cd9ea86a1a0e2 [following]\n",
            "--2024-03-26 01:31:59--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/35948150/EGFR.sdf.xz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240326/eu-west-1/s3/aws4_request&X-Amz-Date=20240326T013158Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=bdc4bfec8c1d208b9030b9ddbad80393ce461404d0828d6e8c8cd9ea86a1a0e2\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.36.170, 52.218.24.251, 52.218.116.64, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.36.170|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62175752 (59M) [application/x-xz]\n",
            "Saving to: ‘data/EGFR.sdf.xz’\n",
            "\n",
            "data/EGFR.sdf.xz    100%[===================>]  59.29M  16.8MB/s    in 4.0s    \n",
            "\n",
            "2024-03-26 01:32:03 (14.9 MB/s) - ‘data/EGFR.sdf.xz’ saved [62175752/62175752]\n",
            "\n",
            "unxz: data/EGFR.sdf: File exists\n",
            "--2024-03-26 01:32:03--  https://figshare.com/ndownloader/files/35948195\n",
            "Resolving figshare.com (figshare.com)... 54.76.53.103, 54.73.41.93, 2a05:d018:1f4:d003:8a09:a9ea:f161:d4fe, ...\n",
            "Connecting to figshare.com (figshare.com)|54.76.53.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/35948195/KDR.sdf.xz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240326/eu-west-1/s3/aws4_request&X-Amz-Date=20240326T013204Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=94274a742f89ced23f6f625fb01d96c91f55c1c75e8165f69e8c01e5a0fd017b [following]\n",
            "--2024-03-26 01:32:04--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/35948195/KDR.sdf.xz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240326/eu-west-1/s3/aws4_request&X-Amz-Date=20240326T013204Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=94274a742f89ced23f6f625fb01d96c91f55c1c75e8165f69e8c01e5a0fd017b\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.92.16.48, 52.92.33.224, 52.218.101.147, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.92.16.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63162552 (60M) [application/x-xz]\n",
            "Saving to: ‘data/KDR.sdf.xz’\n",
            "\n",
            "data/KDR.sdf.xz     100%[===================>]  60.24M  14.7MB/s    in 4.7s    \n",
            "\n",
            "2024-03-26 01:32:10 (12.9 MB/s) - ‘data/KDR.sdf.xz’ saved [63162552/63162552]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data\n",
        "!wget -O data/FGFR1.sdf.xz https://figshare.com/ndownloader/files/35948165\n",
        "!unxz data/FGFR1.sdf.xz\n",
        "\n",
        "!wget -O data/EGFR.sdf.xz https://figshare.com/ndownloader/files/35948150\n",
        "!unxz data/EGFR.sdf.xz\n",
        "\n",
        "!wget -O data/KDR.sdf.xz https://figshare.com/ndownloader/files/35948195\n",
        "!unxz data/KDR.sdf.xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNPHgz-fW_Pp",
        "outputId": "16766a71-6e0e-4063-885d-7957e5b3c4a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of sample molecule: <class 'rdkit.Chem.rdchem.Mol'>\n",
            "Num conformers: 1\n"
          ]
        }
      ],
      "source": [
        "from rdkit import Chem\n",
        "\n",
        "# Function to load molecules from an SDF file and return them in a dictionary indexed by InChI key\n",
        "def load_molecules_from_sdf(sdf_file):\n",
        "    with Chem.SDMolSupplier(sdf_file) as supplier:\n",
        "        mols = {mol.GetProp('_Name'): mol for mol in supplier if mol is not None}\n",
        "    return mols\n",
        "\n",
        "# Load molecules for each target\n",
        "fgfr1_mols = load_molecules_from_sdf(\"./data/FGFR1.sdf\")\n",
        "egfr_mols = load_molecules_from_sdf(\"./data/EGFR.sdf\")\n",
        "kdr_mols = load_molecules_from_sdf(\"./data/KDR.sdf\")\n",
        "\n",
        "# Example: Print basic information for a sample molecule from FGFR1\n",
        "sample_mol = list(fgfr1_mols.values())[0]  # Just an example, adjust based on actual data\n",
        "print(f\"Type of sample molecule: {type(sample_mol)}\")\n",
        "print(f\"Num conformers: {sample_mol.GetNumConformers()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -q -y -c conda-forge -c schrodinger pymol-bundle\n",
        "!pip install botorch gpytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keEx9wVLRo62",
        "outputId": "5ecb90ae-08dc-477b-d706-3f03f4ddf6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            " - schrodinger\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pymol-bundle\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    alsa-lib-1.2.3.2           |       h166bdaf_0         554 KB  conda-forge\n",
            "    apbs-1.5                   |       hd8ae8d1_0        22.8 MB  conda-forge\n",
            "    biopython-1.83             |  py310h2372a71_0         2.6 MB  conda-forge\n",
            "    boost-cpp-1.74.0           |       h6cacc03_7        16.3 MB  conda-forge\n",
            "    cached-property-1.5.2      |       hd8ed1ab_1           4 KB  conda-forge\n",
            "    cached_property-1.5.2      |     pyha770c72_1          11 KB  conda-forge\n",
            "    cairo-1.16.0               |    ha12eb4b_1010         1.5 MB  conda-forge\n",
            "    collada2gltf-2.1.4         |       h6bb024c_0         3.2 MB  schrodinger\n",
            "    conda-23.9.0               |  py310hff52083_2         946 KB  conda-forge\n",
            "    conda-libmamba-solver-23.9.3|     pyhd8ed1ab_0          45 KB  conda-forge\n",
            "    cryptography-41.0.3        |  py310h130f0dd_0         2.0 MB\n",
            "    curl-8.2.1                 |       h37d81fd_0          83 KB\n",
            "    dbus-1.13.6                |       h5008d03_3         604 KB  conda-forge\n",
            "    glew-2.1.0                 |       h9c3ff4c_2         647 KB  conda-forge\n",
            "    glib-2.78.4                |       hfc55251_0         478 KB  conda-forge\n",
            "    glib-tools-2.78.4          |       hfc55251_0         109 KB  conda-forge\n",
            "    gst-plugins-base-1.20.2    |       hcf0ee16_0         2.8 MB  conda-forge\n",
            "    gstreamer-1.20.3           |       hd4edc92_2         2.0 MB  conda-forge\n",
            "    h5py-3.8.0                 |nompi_py310h0311031_100         1.1 MB  conda-forge\n",
            "    hdf4-4.2.15                |       h9772cbc_5         951 KB  conda-forge\n",
            "    hdf5-1.12.2                |nompi_h2386368_101         3.2 MB  conda-forge\n",
            "    icu-69.1                   |       h9c3ff4c_0        13.2 MB  conda-forge\n",
            "    jpeg-9e                    |       h166bdaf_2         269 KB  conda-forge\n",
            "    keyutils-1.6.1             |       h166bdaf_0         115 KB  conda-forge\n",
            "    krb5-1.20.1                |       hf9c8cef_0         1.3 MB  conda-forge\n",
            "    lcms2-2.15                 |       hfd0df8a_0         235 KB  conda-forge\n",
            "    libaec-1.1.3               |       h59595ed_0          35 KB  conda-forge\n",
            "    libarchive-3.6.2           |       hab531cd_0         892 KB\n",
            "    libclang-13.0.1            |default_h7634d5b_6         9.1 MB  conda-forge\n",
            "    libcurl-8.2.1              |       h91b91d3_0         398 KB\n",
            "    libdeflate-1.17            |       h0b41bf4_0          63 KB  conda-forge\n",
            "    libevent-2.1.10            |       h9b69904_4         1.1 MB  conda-forge\n",
            "    libglu-9.0.0               |    he1b5a44_1001         413 KB  conda-forge\n",
            "    libholoplaycore-0.1.0_rc4  |                1         325 KB  schrodinger\n",
            "    libjpeg-turbo-2.1.4        |       h166bdaf_0         988 KB  conda-forge\n",
            "    libllvm13-13.0.1           |       hf817b99_2        33.6 MB  conda-forge\n",
            "    libmamba-1.5.1             |       hba0046a_0         1.8 MB\n",
            "    libmambapy-1.5.1           |  py310ha06983f_0         311 KB\n",
            "    libnetcdf-4.8.1            |nompi_h21705cb_104         1.5 MB  conda-forge\n",
            "    libnghttp2-1.52.0          |       ha637b67_1         671 KB\n",
            "    libogg-1.3.4               |       h7f98852_1         206 KB  conda-forge\n",
            "    libopus-1.3.1              |       h7f98852_1         255 KB  conda-forge\n",
            "    libpq-14.5                 |       h2baec63_5         2.3 MB  conda-forge\n",
            "    libssh2-1.10.0             |       haa6b8db_3         234 KB  conda-forge\n",
            "    libtiff-4.5.0              |       h6adf6a1_2         397 KB  conda-forge\n",
            "    libvorbis-1.3.7            |       h9c3ff4c_0         280 KB  conda-forge\n",
            "    libxkbcommon-1.0.3         |       he3ba5ed_0         581 KB  conda-forge\n",
            "    libxml2-2.9.14             |       haae042b_4         720 KB  conda-forge\n",
            "    libzip-1.9.2               |       hc869a4a_1          97 KB  conda-forge\n",
            "    mengine-1                  |       h14c3975_1         676 KB  schrodinger\n",
            "    mmcif_pdbx-2.0.1           |     pyhd8ed1ab_0          25 KB  conda-forge\n",
            "    mpeg_encode-1              |       h14c3975_1         106 KB  schrodinger\n",
            "    mtz2ccp4_px-1.1            |       h3218e01_0         395 KB  schrodinger\n",
            "    mysql-common-8.0.32        |       h14678bc_0         784 KB  conda-forge\n",
            "    mysql-libs-8.0.32          |       h54cf53e_0         1.5 MB  conda-forge\n",
            "    nspr-4.35                  |       h27087fc_0         222 KB  conda-forge\n",
            "    nss-3.98                   |       h1d7d5a4_0         1.9 MB  conda-forge\n",
            "    openbabel-3.1.1            |  py310h4b1c3e3_4         5.6 MB  conda-forge\n",
            "    openssl-1.1.1w             |       hd590300_0         1.9 MB  conda-forge\n",
            "    pdb2pqr-3.6.1              |     pyhd8ed1ab_0         150 KB  conda-forge\n",
            "    pillow-9.4.0               |  py310h023d228_1        44.3 MB  conda-forge\n",
            "    pmw-2.0.1                  |py310hff52083_1008         517 KB  conda-forge\n",
            "    propka-3.5.1               |     pyhc1e730c_1          93 KB  conda-forge\n",
            "    pycollada-0.8              |     pyhd8ed1ab_0          89 KB  conda-forge\n",
            "    pykerberos-1.2.4           |  py310hd1ceca7_3          26 KB  conda-forge\n",
            "    pymol-2.5.7                |  py310had9a3ec_0         9.1 MB  schrodinger\n",
            "    pymol-bundle-2.5.7         |                1          17 KB  schrodinger\n",
            "    pymol-web-examples-2.4     |                1         1.9 MB  schrodinger\n",
            "    pyopenssl-23.2.0           |     pyhd8ed1ab_1         126 KB  conda-forge\n",
            "    pyqt-5.12.3                |  py310hff52083_8          22 KB  conda-forge\n",
            "    pyqt-impl-5.12.3           |  py310h1f8e252_8         5.9 MB  conda-forge\n",
            "    pyqt5-sip-4.19.18          |  py310h122e73d_8         312 KB  conda-forge\n",
            "    pyqtchart-5.12             |  py310hfcd6d55_8         254 KB  conda-forge\n",
            "    pyqtwebengine-5.12.1       |  py310hfcd6d55_8         175 KB  conda-forge\n",
            "    python-3.10.8              |h257c98d_0_cpython        22.1 MB  conda-forge\n",
            "    qt-5.12.9                  |       h1304e3e_6        98.6 MB  conda-forge\n",
            "    rigimol-1.3                |                2         489 KB  schrodinger\n",
            "    ruamel.yaml-0.17.40        |  py310h2372a71_0         196 KB  conda-forge\n",
            "    vtk-m-1.8.0                |       h3fd9d12_2        12.0 MB  schrodinger\n",
            "    yaml-cpp-0.7.0             |       h59595ed_3         203 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       342.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  alsa-lib           conda-forge/linux-64::alsa-lib-1.2.3.2-h166bdaf_0 \n",
            "  apbs               conda-forge/linux-64::apbs-1.5-hd8ae8d1_0 \n",
            "  biopython          conda-forge/linux-64::biopython-1.83-py310h2372a71_0 \n",
            "  cached-property    conda-forge/noarch::cached-property-1.5.2-hd8ed1ab_1 \n",
            "  cached_property    conda-forge/noarch::cached_property-1.5.2-pyha770c72_1 \n",
            "  collada2gltf       schrodinger/linux-64::collada2gltf-2.1.4-h6bb024c_0 \n",
            "  curl               pkgs/main/linux-64::curl-8.2.1-h37d81fd_0 \n",
            "  dbus               conda-forge/linux-64::dbus-1.13.6-h5008d03_3 \n",
            "  glew               conda-forge/linux-64::glew-2.1.0-h9c3ff4c_2 \n",
            "  glib               conda-forge/linux-64::glib-2.78.4-hfc55251_0 \n",
            "  glib-tools         conda-forge/linux-64::glib-tools-2.78.4-hfc55251_0 \n",
            "  gst-plugins-base   conda-forge/linux-64::gst-plugins-base-1.20.2-hcf0ee16_0 \n",
            "  gstreamer          conda-forge/linux-64::gstreamer-1.20.3-hd4edc92_2 \n",
            "  h5py               conda-forge/linux-64::h5py-3.8.0-nompi_py310h0311031_100 \n",
            "  hdf4               conda-forge/linux-64::hdf4-4.2.15-h9772cbc_5 \n",
            "  hdf5               conda-forge/linux-64::hdf5-1.12.2-nompi_h2386368_101 \n",
            "  jpeg               conda-forge/linux-64::jpeg-9e-h166bdaf_2 \n",
            "  keyutils           conda-forge/linux-64::keyutils-1.6.1-h166bdaf_0 \n",
            "  libaec             conda-forge/linux-64::libaec-1.1.3-h59595ed_0 \n",
            "  libclang           conda-forge/linux-64::libclang-13.0.1-default_h7634d5b_6 \n",
            "  libevent           conda-forge/linux-64::libevent-2.1.10-h9b69904_4 \n",
            "  libglu             conda-forge/linux-64::libglu-9.0.0-he1b5a44_1001 \n",
            "  libholoplaycore    schrodinger/linux-64::libholoplaycore-0.1.0_rc4-1 \n",
            "  libllvm13          conda-forge/linux-64::libllvm13-13.0.1-hf817b99_2 \n",
            "  libnetcdf          conda-forge/linux-64::libnetcdf-4.8.1-nompi_h21705cb_104 \n",
            "  libogg             conda-forge/linux-64::libogg-1.3.4-h7f98852_1 \n",
            "  libopus            conda-forge/linux-64::libopus-1.3.1-h7f98852_1 \n",
            "  libpq              conda-forge/linux-64::libpq-14.5-h2baec63_5 \n",
            "  libvorbis          conda-forge/linux-64::libvorbis-1.3.7-h9c3ff4c_0 \n",
            "  libxkbcommon       conda-forge/linux-64::libxkbcommon-1.0.3-he3ba5ed_0 \n",
            "  libzip             conda-forge/linux-64::libzip-1.9.2-hc869a4a_1 \n",
            "  mengine            schrodinger/linux-64::mengine-1-h14c3975_1 \n",
            "  mmcif_pdbx         conda-forge/noarch::mmcif_pdbx-2.0.1-pyhd8ed1ab_0 \n",
            "  mpeg_encode        schrodinger/linux-64::mpeg_encode-1-h14c3975_1 \n",
            "  mtz2ccp4_px        schrodinger/linux-64::mtz2ccp4_px-1.1-h3218e01_0 \n",
            "  mysql-common       conda-forge/linux-64::mysql-common-8.0.32-h14678bc_0 \n",
            "  mysql-libs         conda-forge/linux-64::mysql-libs-8.0.32-h54cf53e_0 \n",
            "  nspr               conda-forge/linux-64::nspr-4.35-h27087fc_0 \n",
            "  nss                conda-forge/linux-64::nss-3.98-h1d7d5a4_0 \n",
            "  pdb2pqr            conda-forge/noarch::pdb2pqr-3.6.1-pyhd8ed1ab_0 \n",
            "  pmw                conda-forge/linux-64::pmw-2.0.1-py310hff52083_1008 \n",
            "  propka             conda-forge/noarch::propka-3.5.1-pyhc1e730c_1 \n",
            "  pycollada          conda-forge/noarch::pycollada-0.8-pyhd8ed1ab_0 \n",
            "  pykerberos         conda-forge/linux-64::pykerberos-1.2.4-py310hd1ceca7_3 \n",
            "  pymol              schrodinger/linux-64::pymol-2.5.7-py310had9a3ec_0 \n",
            "  pymol-bundle       schrodinger/linux-64::pymol-bundle-2.5.7-1 \n",
            "  pymol-web-examples schrodinger/noarch::pymol-web-examples-2.4-1 \n",
            "  pyopenssl          conda-forge/noarch::pyopenssl-23.2.0-pyhd8ed1ab_1 \n",
            "  pyqt               conda-forge/linux-64::pyqt-5.12.3-py310hff52083_8 \n",
            "  pyqt-impl          conda-forge/linux-64::pyqt-impl-5.12.3-py310h1f8e252_8 \n",
            "  pyqt5-sip          conda-forge/linux-64::pyqt5-sip-4.19.18-py310h122e73d_8 \n",
            "  pyqtchart          conda-forge/linux-64::pyqtchart-5.12-py310hfcd6d55_8 \n",
            "  pyqtwebengine      conda-forge/linux-64::pyqtwebengine-5.12.1-py310hfcd6d55_8 \n",
            "  qt                 conda-forge/linux-64::qt-5.12.9-h1304e3e_6 \n",
            "  rigimol            schrodinger/linux-64::rigimol-1.3-2 \n",
            "  vtk-m              schrodinger/linux-64::vtk-m-1.8.0-h3fd9d12_2 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  libssh2              pkgs/main::libssh2-1.10.0-hdbd6064_2 --> conda-forge::libssh2-1.10.0-haa6b8db_3 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  conda-libmamba-so~ pkgs/main::conda-libmamba-solver-23.1~ --> conda-forge::conda-libmamba-solver-23.9.3-pyhd8ed1ab_0 \n",
            "  cryptography       conda-forge::cryptography-42.0.5-py31~ --> pkgs/main::cryptography-41.0.3-py310h130f0dd_0 \n",
            "  krb5                    pkgs/main::krb5-1.20.1-h143b758_1 --> conda-forge::krb5-1.20.1-hf9c8cef_0 \n",
            "  yaml-cpp             pkgs/main::yaml-cpp-0.8.0-h6a678d5_0 --> conda-forge::yaml-cpp-0.7.0-h59595ed_3 \n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  boost-cpp                               1.74.0-h75c5d50_8 --> 1.74.0-h6cacc03_7 \n",
            "  cairo                                1.16.0-ha61ee94_1014 --> 1.16.0-ha12eb4b_1010 \n",
            "  conda                              24.3.0-py310hff52083_0 --> 23.9.0-py310hff52083_2 \n",
            "  icu                                       70.1-h27087fc_0 --> 69.1-h9c3ff4c_0 \n",
            "  lcms2                                     2.15-haa2dc70_1 --> 2.15-hfd0df8a_0 \n",
            "  libarchive                               3.6.2-h6ac8c49_2 --> 3.6.2-hab531cd_0 \n",
            "  libcurl                                  8.5.0-h251f7ec_0 --> 8.2.1-h91b91d3_0 \n",
            "  libdeflate                                1.18-h0b41bf4_0 --> 1.17-h0b41bf4_0 \n",
            "  libjpeg-turbo                          2.1.5.1-hd590300_1 --> 2.1.4-h166bdaf_0 \n",
            "  libmamba                                 1.5.3-haf1ee3a_0 --> 1.5.1-hba0046a_0 \n",
            "  libmambapy                          1.5.3-py310h2dafd23_0 --> 1.5.1-py310ha06983f_0 \n",
            "  libnghttp2                              1.57.0-h2d74bed_0 --> 1.52.0-ha637b67_1 \n",
            "  libtiff                                  4.5.1-h8b53f26_1 --> 4.5.0-h6adf6a1_2 \n",
            "  libxml2                                 2.10.3-hca2bb57_4 --> 2.9.14-haae042b_4 \n",
            "  openbabel                           3.1.1-py310heaf86c6_5 --> 3.1.1-py310h4b1c3e3_4 \n",
            "  openssl                                  3.2.1-hd590300_1 --> 1.1.1w-hd590300_0 \n",
            "  pillow                              9.5.0-py310h065c6d2_0 --> 9.4.0-py310h023d228_1 \n",
            "  python                         3.10.14-hd12c33a_0_cpython --> 3.10.8-h257c98d_0_cpython \n",
            "  ruamel.yaml                        0.18.6-py310h2372a71_0 --> 0.17.40-py310h2372a71_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting botorch\n",
            "  Downloading botorch-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.11-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting multipledispatch (from botorch)\n",
            "  Downloading multipledispatch-1.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting scipy (from botorch)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpmath<=1.3,>=0.19 (from botorch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting torch>=1.13.1 (from botorch)\n",
            "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting pyro-ppl>=1.8.4 (from botorch)\n",
            "  Downloading pyro_ppl-1.9.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting linear-operator==0.5.1 (from botorch)\n",
            "  Downloading linear_operator-0.5.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting scikit-learn (from gpytorch)\n",
            "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting jaxtyping>=0.2.9 (from linear-operator==0.5.1->botorch)\n",
            "  Downloading jaxtyping-0.2.28-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typeguard~=2.13.3 (from linear-operator==0.5.1->botorch)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/site-packages (from pyro-ppl>=1.8.4->botorch) (1.26.4)\n",
            "Collecting opt-einsum>=2.3.2 (from pyro-ppl>=1.8.4->botorch)\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl>=1.8.4->botorch)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/site-packages (from pyro-ppl>=1.8.4->botorch) (4.66.2)\n",
            "Collecting filelock (from torch>=1.13.1->botorch)\n",
            "  Downloading filelock-3.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.1->botorch) (4.10.0)\n",
            "Collecting sympy (from torch>=1.13.1->botorch)\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch>=1.13.1->botorch)\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting jinja2 (from torch>=1.13.1->botorch)\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting fsspec (from torch>=1.13.1->botorch)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch>=1.13.1->botorch)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->botorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn->gpytorch)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn->gpytorch)\n",
            "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.13.1->botorch)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Downloading botorch-0.10.0-py3-none-any.whl (613 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.1/613.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gpytorch-1.11-py3-none-any.whl (266 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.5.1-py3-none-any.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.5/174.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_ppl-1.9.0-py3-none-any.whl (745 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m745.2/745.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multipledispatch-1.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.2.28-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Downloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading filelock-3.13.3-py3-none-any.whl (11 kB)\n",
            "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyro-api, multipledispatch, mpmath, typeguard, threadpoolctl, sympy, scipy, opt-einsum, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, joblib, fsspec, filelock, triton, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, jaxtyping, nvidia-cusolver-cu12, torch, pyro-ppl, linear-operator, gpytorch, botorch\n",
            "Successfully installed MarkupSafe-2.1.5 botorch-0.10.0 filelock-3.13.3 fsspec-2024.3.1 gpytorch-1.11 jaxtyping-0.2.28 jinja2-3.1.3 joblib-1.3.2 linear-operator-0.5.1 mpmath-1.3.0 multipledispatch-1.0.0 networkx-3.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 opt-einsum-3.3.0 pyro-api-0.1.2 pyro-ppl-1.9.0 scikit-learn-1.4.1.post1 scipy-1.12.0 sympy-1.12 threadpoolctl-3.4.0 torch-2.2.1 triton-2.2.0 typeguard-2.13.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJTx6FT1apEI",
        "outputId": "b1fae7d8-993b-4803-b33c-04800b64e1f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target = load_target('FGFR1')\n",
        "target.view(sample_mol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhKQU_0iXDFQ"
      },
      "source": [
        "# BO Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v1sRMZSJx5x"
      },
      "outputs": [],
      "source": [
        "import botorch\n",
        "import gpytorch\n",
        "import numpy as np\n",
        "import torch\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "from gpytorch.kernels import InducingPointKernel, Kernel, ScaleKernel\n",
        "from gpytorch.models import ExactGP\n",
        "\n",
        "def batch_tanimoto_sim(x1: torch.Tensor, x2: torch.Tensor):\n",
        "    \"\"\"tanimoto between two batched tensors, across last 2 dimensions\"\"\"\n",
        "    assert x1.ndim >= 2 and x2.ndim >= 2\n",
        "    dot_prod = torch.matmul(x1, torch.transpose(x2, -1, -2))\n",
        "    x1_sum = torch.sum(x1**2, dim=-1, keepdims=True)\n",
        "    x2_sum = torch.sum(x2**2, dim=-1, keepdims=True)\n",
        "    return (dot_prod) / (x1_sum + torch.transpose(x2_sum, -1, -2) - dot_prod)\n",
        "\n",
        "\n",
        "class TanimotoKernel(Kernel):\n",
        "    \"\"\"Tanimoto coefficient kernel\"\"\"\n",
        "\n",
        "    is_stationary = False\n",
        "    has_lengthscale = False\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TanimotoKernel, self).__init__(**kwargs)\n",
        "\n",
        "    def forward(self, x1, x2, diag=False, **params):\n",
        "        if diag:\n",
        "            assert x1.size() == x2.size() and torch.equal(x1, x2)\n",
        "            return torch.ones(*x1.shape[:-2], x1.shape[-2], dtype=x1.dtype, device=x1.device)\n",
        "        return batch_tanimoto_sim(x1, x2)\n",
        "\n",
        "\n",
        "class TanimotoGP(ExactGP, botorch.models.gpytorch.GPyTorchModel):\n",
        "    _num_outputs = 1  # looks like botorch needs this\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_x,\n",
        "        train_y,\n",
        "        likelihood=None,\n",
        "    ):\n",
        "        # Fill in likelihood\n",
        "        if likelihood is None:\n",
        "            likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "\n",
        "        botorch.models.gpytorch.GPyTorchModel.__init__(self)\n",
        "        ExactGP.__init__(self, train_x, train_y, likelihood)\n",
        "\n",
        "        self.covar_module = ScaleKernel(TanimotoKernel())\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Normal mean + covar\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "    @property\n",
        "    def hparam_dict(self):\n",
        "        return {\n",
        "            \"likelihood.noise\": self.likelihood.noise.item(),\n",
        "            \"covar_module.outputscale\": self.covar_module.outputscale.item(),\n",
        "            \"mean_module.constant\": self.mean_module.constant.item(),\n",
        "        }\n",
        "\n",
        "\n",
        "class TanimotoSGP(TanimotoGP):\n",
        "    \"\"\"SGPR with Tanimoto GP\"\"\"\n",
        "\n",
        "    def __init__(self, *args, inducing_points=None, **kwargs):\n",
        "        assert inducing_points is not None\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # now use a base covar module\n",
        "        self.base_covar_module = self.covar_module\n",
        "        self.covar_module = InducingPointKernel(\n",
        "            self.base_covar_module,\n",
        "            inducing_points=inducing_points,\n",
        "            likelihood=self.likelihood,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def hparam_dict(self):\n",
        "        return {\n",
        "            \"likelihood.noise\": self.likelihood.noise.item(),\n",
        "            \"covar_module.base_kernel.outputscale\": self.covar_module.base_kernel.outputscale.item(),\n",
        "            \"mean_module.constant\": self.mean_module.constant.item(),\n",
        "        }\n",
        "\n",
        "def batch_predict_mu_var_numpy(\n",
        "    gp_model: ExactGP,\n",
        "    x: torch.Tensor,\n",
        "    batch_size: int = 2048,\n",
        "    include_var: bool = True,\n",
        "):\n",
        "    \"\"\"Utility function to predict mean/variance of GP\"\"\"\n",
        "    gp_model.eval()\n",
        "    mu = []\n",
        "    var = []\n",
        "    with gpytorch.settings.fast_computations(False, False, False), torch.no_grad():\n",
        "        for batch_start in range(0, len(x), batch_size):\n",
        "            batch_end = batch_start + batch_size\n",
        "            output = gp_model(x[batch_start:batch_end])\n",
        "            mu_batch = output.mean.detach().cpu().numpy()\n",
        "            if include_var:\n",
        "                var_batch = output.variance.detach().cpu().numpy()\n",
        "            else:\n",
        "                var_batch = np.zeros_like(mu_batch)\n",
        "            mu.append(mu_batch)\n",
        "            var.append(var_batch)\n",
        "    mu = np.concatenate(mu, axis=0)\n",
        "    var = np.concatenate(var, axis=0)\n",
        "    return mu, var\n",
        "\n",
        "\n",
        "def fit_gp_hyperparameters(gp_model: ExactGP):\n",
        "    \"\"\"Optimize train MLL to fit GP hyperparameters\"\"\"\n",
        "\n",
        "    mll = ExactMarginalLogLikelihood(gp_model.likelihood, gp_model)\n",
        "    opt_res = botorch.optim.fit.fit_gpytorch_scipy(mll)\n",
        "    return opt_res\n",
        "\n",
        "\n",
        "def transfer_gp_hyperparameters(gp_model_src: TanimotoGP, gp_model_dest: TanimotoGP):\n",
        "    hp_src = gp_model_src.hparam_dict\n",
        "    hp_dest = gp_model_dest.hparam_dict\n",
        "\n",
        "    # Clear caches by putting into train mode\n",
        "    gp_model_dest.train()\n",
        "\n",
        "    # Do the transfer\n",
        "    for key in hp_dest.keys():\n",
        "        param_name = key.split(\".\")[-1]\n",
        "        for src_key, val in hp_src.items():\n",
        "            src_param_name = src_key.split(\".\")[-1]\n",
        "            if src_param_name == param_name:\n",
        "                hp_dest[key] = val\n",
        "\n",
        "    # Reinitialize variables\n",
        "    gp_model_dest.initialize(**hp_dest)\n",
        "\n",
        "\"\"\" Some common utility functions \"\"\"\n",
        "class CachedFunction:\n",
        "    \"\"\"\n",
        "    Efficient function which caches previously computed values to avoid repeat computation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, f: callable, cache: dict = None, transform: callable = None):\n",
        "        \"\"\"Init function\n",
        "\n",
        "        :param f: The function to cache\n",
        "        :type f: callable\n",
        "        :param cache: dict mapping known inputs-> outputs of f, defaults to None\n",
        "        :type cache: dict, optional\n",
        "        :param transform: optional transform function to apply to values\n",
        "            of f (e.g. scaling output to be in [0, 1]), defaults to None\n",
        "        :type transform: callable, optional\n",
        "        \"\"\"\n",
        "        self._f = f\n",
        "        self._cache = cache\n",
        "        if self._cache is None:\n",
        "            self._cache = dict()\n",
        "        self.transform = transform\n",
        "\n",
        "    @property\n",
        "    def cache(self):\n",
        "        return self._cache\n",
        "\n",
        "    def _batch_f_eval(self, input_list):\n",
        "        return [self._f(x) for x in input_list]\n",
        "\n",
        "    def _batch_transform(self, output_list):\n",
        "        if self.transform is None:\n",
        "            return output_list\n",
        "        else:\n",
        "            return [self.transform(x) for x in output_list]\n",
        "\n",
        "    def __call__(self, inputs, batch=False):\n",
        "        # Ensure it is in batch form\n",
        "        if not batch:\n",
        "            inputs = [inputs]\n",
        "\n",
        "        # Eval function at non-cached inputs\n",
        "        inputs_not_cached = [x for x in inputs if x not in self.cache]\n",
        "        outputs_not_cached = self._batch_f_eval(inputs_not_cached)\n",
        "\n",
        "        # Add new values to cache\n",
        "        for x, y in zip(inputs_not_cached, outputs_not_cached):\n",
        "            self._cache[x] = y\n",
        "\n",
        "        # Get and transform outputs\n",
        "        outputs = [self._cache[x] for x in inputs]\n",
        "        outputs = self._batch_transform(outputs)\n",
        "\n",
        "        # Undo batching if desired\n",
        "        if not batch:\n",
        "            assert len(outputs) == 1\n",
        "            outputs = outputs[0]\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class CachedBatchFunction(CachedFunction):\n",
        "    \"\"\"\n",
        "    Special kind of cached function where f takes a batch of inputs\n",
        "    instead of a single input. The input to the underlying function\n",
        "    (and transform function) will always be a batch of data.\n",
        "    \"\"\"\n",
        "\n",
        "    def _batch_f_eval(self, input_list):\n",
        "        return self._f(input_list)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3YFQqH5Kuym"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "from rdkit import Chem, rdBase\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "rdBase.DisableLog(\"rdApp.error\")\n",
        "\n",
        "\n",
        "def cut(mol):\n",
        "    if not mol.HasSubstructMatch(Chem.MolFromSmarts(\"[*]-;!@[*]\")):\n",
        "        return None\n",
        "\n",
        "    bis = random.choice(mol.GetSubstructMatches(Chem.MolFromSmarts(\"[*]-;!@[*]\")))  # single bond not in ring\n",
        "\n",
        "    bs = [mol.GetBondBetweenAtoms(bis[0], bis[1]).GetIdx()]\n",
        "\n",
        "    fragments_mol = Chem.FragmentOnBonds(mol, bs, addDummies=True, dummyLabels=[(1, 1)])\n",
        "\n",
        "    try:\n",
        "        return Chem.GetMolFrags(fragments_mol, asMols=True, sanitizeFrags=True)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def cut_ring(mol):\n",
        "    for i in range(10):\n",
        "        if random.random() < 0.5:\n",
        "            if not mol.HasSubstructMatch(Chem.MolFromSmarts(\"[R]@[R]@[R]@[R]\")):\n",
        "                return None\n",
        "            bis = random.choice(mol.GetSubstructMatches(Chem.MolFromSmarts(\"[R]@[R]@[R]@[R]\")))\n",
        "            bis = (\n",
        "                (bis[0], bis[1]),\n",
        "                (bis[2], bis[3]),\n",
        "            )\n",
        "        else:\n",
        "            if not mol.HasSubstructMatch(Chem.MolFromSmarts(\"[R]@[R;!D2]@[R]\")):\n",
        "                return None\n",
        "            bis = random.choice(mol.GetSubstructMatches(Chem.MolFromSmarts(\"[R]@[R;!D2]@[R]\")))\n",
        "            bis = (\n",
        "                (bis[0], bis[1]),\n",
        "                (bis[1], bis[2]),\n",
        "            )\n",
        "\n",
        "        bs = [mol.GetBondBetweenAtoms(x, y).GetIdx() for x, y in bis]\n",
        "\n",
        "        fragments_mol = Chem.FragmentOnBonds(mol, bs, addDummies=True, dummyLabels=[(1, 1), (1, 1)])\n",
        "\n",
        "        try:\n",
        "            fragments = Chem.GetMolFrags(fragments_mol, asMols=True, sanitizeFrags=True)\n",
        "            if len(fragments) == 2:\n",
        "                return fragments\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def ring_OK(mol):\n",
        "    if not mol.HasSubstructMatch(Chem.MolFromSmarts(\"[R]\")):\n",
        "        return True\n",
        "\n",
        "    ring_allene = mol.HasSubstructMatch(Chem.MolFromSmarts(\"[R]=[R]=[R]\"))\n",
        "\n",
        "    cycle_list = mol.GetRingInfo().AtomRings()\n",
        "    max_cycle_length = max([len(j) for j in cycle_list])\n",
        "    macro_cycle = max_cycle_length > 6\n",
        "\n",
        "    double_bond_in_small_ring = mol.HasSubstructMatch(Chem.MolFromSmarts(\"[r3,r4]=[r3,r4]\"))\n",
        "\n",
        "    return not ring_allene and not macro_cycle and not double_bond_in_small_ring\n",
        "\n",
        "\n",
        "def mol_ok(\n",
        "    mol,\n",
        "    min_num_atoms=5,\n",
        "    mean_num_atoms=40.0,\n",
        "    std_num_atoms=10.0,\n",
        "):\n",
        "    try:\n",
        "        Chem.SanitizeMol(mol)\n",
        "        target_size = std_num_atoms * np.random.randn() + mean_num_atoms  # parameters set in GA_mol\n",
        "        if mol.GetNumAtoms() > min_num_atoms and mol.GetNumAtoms() < target_size:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def crossover_ring(parent_A, parent_B, **mol_ok_kwargs):\n",
        "    ring_smarts = Chem.MolFromSmarts(\"[R]\")\n",
        "    if not parent_A.HasSubstructMatch(ring_smarts) and not parent_B.HasSubstructMatch(ring_smarts):\n",
        "        return None\n",
        "\n",
        "    rxn_smarts1 = [\n",
        "        \"[*:1]~[1*].[1*]~[*:2]>>[*:1]-[*:2]\",\n",
        "        \"[*:1]~[1*].[1*]~[*:2]>>[*:1]=[*:2]\",\n",
        "    ]\n",
        "    rxn_smarts2 = [\n",
        "        \"([*:1]~[1*].[1*]~[*:2])>>[*:1]-[*:2]\",\n",
        "        \"([*:1]~[1*].[1*]~[*:2])>>[*:1]=[*:2]\",\n",
        "    ]\n",
        "\n",
        "    for i in range(10):\n",
        "        fragments_A = cut_ring(parent_A)\n",
        "        fragments_B = cut_ring(parent_B)\n",
        "\n",
        "        if fragments_A is None or fragments_B is None:\n",
        "            return None\n",
        "\n",
        "        new_mol_trial = []\n",
        "        for rs in rxn_smarts1:\n",
        "            rxn1 = AllChem.ReactionFromSmarts(rs)\n",
        "            new_mol_trial = []\n",
        "            for fa in fragments_A:\n",
        "                for fb in fragments_B:\n",
        "                    new_mol_trial.append(rxn1.RunReactants((fa, fb))[0])\n",
        "\n",
        "        new_mols = []\n",
        "        for rs in rxn_smarts2:\n",
        "            rxn2 = AllChem.ReactionFromSmarts(rs)\n",
        "            for m in new_mol_trial:\n",
        "                m = m[0]\n",
        "                if mol_ok(m, **mol_ok_kwargs):\n",
        "                    new_mols += list(rxn2.RunReactants((m,)))\n",
        "\n",
        "        new_mols2 = []\n",
        "        for m in new_mols:\n",
        "            m = m[0]\n",
        "            if mol_ok(m, **mol_ok_kwargs) and ring_OK(m):\n",
        "                new_mols2.append(m)\n",
        "\n",
        "        if len(new_mols2) > 0:\n",
        "            return random.choice(new_mols2)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def crossover_non_ring(parent_A, parent_B, **mol_ok_kwargs):\n",
        "    for i in range(10):\n",
        "        fragments_A = cut(parent_A)\n",
        "        fragments_B = cut(parent_B)\n",
        "        if fragments_A is None or fragments_B is None:\n",
        "            return None\n",
        "        rxn = AllChem.ReactionFromSmarts(\"[*:1]-[1*].[1*]-[*:2]>>[*:1]-[*:2]\")\n",
        "        new_mol_trial = []\n",
        "        for fa in fragments_A:\n",
        "            for fb in fragments_B:\n",
        "                new_mol_trial.append(rxn.RunReactants((fa, fb))[0])\n",
        "\n",
        "        new_mols = []\n",
        "        for mol in new_mol_trial:\n",
        "            mol = mol[0]\n",
        "            if mol_ok(mol, **mol_ok_kwargs):\n",
        "                new_mols.append(mol)\n",
        "\n",
        "        if len(new_mols) > 0:\n",
        "            return random.choice(new_mols)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def crossover(parent_A, parent_B, **mol_ok_kwargs):\n",
        "    parent_smiles = [Chem.MolToSmiles(parent_A), Chem.MolToSmiles(parent_B)]\n",
        "    try:\n",
        "        Chem.Kekulize(parent_A, clearAromaticFlags=True)\n",
        "        Chem.Kekulize(parent_B, clearAromaticFlags=True)\n",
        "\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    for i in range(10):\n",
        "        if random.random() <= 0.5:\n",
        "            # print 'non-ring crossover'\n",
        "            new_mol = crossover_non_ring(parent_A, parent_B, **mol_ok_kwargs)\n",
        "            if new_mol is not None:\n",
        "                new_smiles = Chem.MolToSmiles(new_mol)\n",
        "                if new_smiles is not None and new_smiles not in parent_smiles:\n",
        "                    return new_mol\n",
        "        else:\n",
        "            # print 'ring crossover'\n",
        "            new_mol = crossover_ring(parent_A, parent_B, **mol_ok_kwargs)\n",
        "            if new_mol is not None:\n",
        "                new_smiles = Chem.MolToSmiles(new_mol)\n",
        "                if new_smiles is not None and new_smiles not in parent_smiles:\n",
        "                    return new_mol\n",
        "\n",
        "    return None\n",
        "def delete_atom():\n",
        "    choices = [\n",
        "        \"[*:1]~[D1:2]>>[*:1]\",\n",
        "        \"[*:1]~[D2:2]~[*:3]>>[*:1]-[*:3]\",\n",
        "        \"[*:1]~[D3:2](~[*;!H0:3])~[*:4]>>[*:1]-[*:3]-[*:4]\",\n",
        "        \"[*:1]~[D4:2](~[*;!H0:3])(~[*;!H0:4])~[*:5]>>[*:1]-[*:3]-[*:4]-[*:5]\",\n",
        "        \"[*:1]~[D4:2](~[*;!H0;!H1:3])(~[*:4])~[*:5]>>[*:1]-[*:3](-[*:4])-[*:5]\",\n",
        "    ]\n",
        "    p = [0.25, 0.25, 0.25, 0.1875, 0.0625]\n",
        "\n",
        "    return np.random.choice(choices, p=p)\n",
        "\n",
        "\n",
        "def append_atom():\n",
        "    choices = [\n",
        "        [\"single\", [\"C\", \"N\", \"O\", \"F\", \"S\", \"Cl\", \"Br\"], 7 * [1.0 / 7.0]],\n",
        "        [\"double\", [\"C\", \"N\", \"O\"], 3 * [1.0 / 3.0]],\n",
        "        [\"triple\", [\"C\", \"N\"], 2 * [1.0 / 2.0]],\n",
        "    ]\n",
        "    p_BO = [0.60, 0.35, 0.05]\n",
        "\n",
        "    index = np.random.choice(list(range(3)), p=p_BO)\n",
        "\n",
        "    BO, atom_list, p = choices[index]\n",
        "    new_atom = np.random.choice(atom_list, p=p)\n",
        "\n",
        "    if BO == \"single\":\n",
        "        rxn_smarts = \"[*;!H0:1]>>[*:1]X\".replace(\"X\", \"-\" + new_atom)\n",
        "    if BO == \"double\":\n",
        "        rxn_smarts = \"[*;!H0;!H1:1]>>[*:1]X\".replace(\"X\", \"=\" + new_atom)\n",
        "    if BO == \"triple\":\n",
        "        rxn_smarts = \"[*;H3:1]>>[*:1]X\".replace(\"X\", \"#\" + new_atom)\n",
        "\n",
        "    return rxn_smarts\n",
        "\n",
        "\n",
        "def insert_atom():\n",
        "    choices = [\n",
        "        [\"single\", [\"C\", \"N\", \"O\", \"S\"], 4 * [1.0 / 4.0]],\n",
        "        [\"double\", [\"C\", \"N\"], 2 * [1.0 / 2.0]],\n",
        "        [\"triple\", [\"C\"], [1.0]],\n",
        "    ]\n",
        "    p_BO = [0.60, 0.35, 0.05]\n",
        "\n",
        "    index = np.random.choice(list(range(3)), p=p_BO)\n",
        "\n",
        "    BO, atom_list, p = choices[index]\n",
        "    new_atom = np.random.choice(atom_list, p=p)\n",
        "\n",
        "    if BO == \"single\":\n",
        "        rxn_smarts = \"[*:1]~[*:2]>>[*:1]X[*:2]\".replace(\"X\", new_atom)\n",
        "    if BO == \"double\":\n",
        "        rxn_smarts = \"[*;!H0:1]~[*:2]>>[*:1]=X-[*:2]\".replace(\"X\", new_atom)\n",
        "    if BO == \"triple\":\n",
        "        rxn_smarts = \"[*;!R;!H1;!H0:1]~[*:2]>>[*:1]#X-[*:2]\".replace(\"X\", new_atom)\n",
        "\n",
        "    return rxn_smarts\n",
        "\n",
        "\n",
        "def change_bond_order():\n",
        "    choices = [\n",
        "        \"[*:1]!-[*:2]>>[*:1]-[*:2]\",\n",
        "        \"[*;!H0:1]-[*;!H0:2]>>[*:1]=[*:2]\",\n",
        "        \"[*:1]#[*:2]>>[*:1]=[*:2]\",\n",
        "        \"[*;!R;!H1;!H0:1]~[*:2]>>[*:1]#[*:2]\",\n",
        "    ]\n",
        "    p = [0.45, 0.45, 0.05, 0.05]\n",
        "\n",
        "    return np.random.choice(choices, p=p)\n",
        "\n",
        "\n",
        "def delete_cyclic_bond():\n",
        "    return \"[*:1]@[*:2]>>([*:1].[*:2])\"\n",
        "\n",
        "\n",
        "def add_ring():\n",
        "    choices = [\n",
        "        \"[*;!r;!H0:1]~[*;!r:2]~[*;!r;!H0:3]>>[*:1]1~[*:2]~[*:3]1\",\n",
        "        \"[*;!r;!H0:1]~[*!r:2]~[*!r:3]~[*;!r;!H0:4]>>[*:1]1~[*:2]~[*:3]~[*:4]1\",\n",
        "        \"[*;!r;!H0:1]~[*!r:2]~[*:3]~[*:4]~[*;!r;!H0:5]>>[*:1]1~[*:2]~[*:3]~[*:4]~[*:5]1\",\n",
        "        \"[*;!r;!H0:1]~[*!r:2]~[*:3]~[*:4]~[*!r:5]~[*;!r;!H0:6]>>[*:1]1~[*:2]~[*:3]~[*:4]~[*:5]~[*:6]1\",\n",
        "    ]\n",
        "    p = [0.05, 0.05, 0.45, 0.45]\n",
        "\n",
        "    return np.random.choice(choices, p=p)\n",
        "\n",
        "\n",
        "def change_atom(mol):\n",
        "    choices = [\"#6\", \"#7\", \"#8\", \"#9\", \"#16\", \"#17\", \"#35\"]\n",
        "    p = [0.15, 0.15, 0.14, 0.14, 0.14, 0.14, 0.14]\n",
        "\n",
        "    X = np.random.choice(choices, p=p)\n",
        "    while not mol.HasSubstructMatch(Chem.MolFromSmarts(\"[\" + X + \"]\")):\n",
        "        X = np.random.choice(choices, p=p)\n",
        "    Y = np.random.choice(choices, p=p)\n",
        "    while Y == X:\n",
        "        Y = np.random.choice(choices, p=p)\n",
        "\n",
        "    return \"[X:1]>>[Y:1]\".replace(\"X\", X).replace(\"Y\", Y)\n",
        "\n",
        "\n",
        "def mutate(mol, mutation_rate):\n",
        "    if random.random() > mutation_rate:\n",
        "        return mol\n",
        "\n",
        "    try:\n",
        "        Chem.Kekulize(mol, clearAromaticFlags=True)\n",
        "    except ValueError:\n",
        "        return mol\n",
        "\n",
        "    p = [0.15, 0.14, 0.14, 0.14, 0.14, 0.14, 0.15]\n",
        "    for i in range(10):\n",
        "        rxn_smarts_list = 7 * [\"\"]\n",
        "        rxn_smarts_list[0] = insert_atom()\n",
        "        rxn_smarts_list[1] = change_bond_order()\n",
        "        rxn_smarts_list[2] = delete_cyclic_bond()\n",
        "        rxn_smarts_list[3] = add_ring()\n",
        "        rxn_smarts_list[4] = delete_atom()\n",
        "        rxn_smarts_list[5] = change_atom(mol)\n",
        "        rxn_smarts_list[6] = append_atom()\n",
        "        rxn_smarts = np.random.choice(rxn_smarts_list, p=p)\n",
        "\n",
        "        # print 'mutation',rxn_smarts\n",
        "\n",
        "        rxn = AllChem.ReactionFromSmarts(rxn_smarts)\n",
        "\n",
        "        new_mol_trial = rxn.RunReactants((mol,))\n",
        "\n",
        "        new_mols = []\n",
        "        for m in new_mol_trial:\n",
        "            m = m[0]\n",
        "            # print Chem.MolToSmiles(mol),mol_ok(mol)\n",
        "            if mol_ok(m) and ring_OK(m):\n",
        "                new_mols.append(m)\n",
        "\n",
        "        if len(new_mols) > 0:\n",
        "            return random.choice(new_mols)\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LGgfVGgLCCF"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem, RDLogger\n",
        "from typing import List, Union\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "ga_logger = logging.getLogger(\"graph_ga\")\n",
        "if len(ga_logger.handlers) == 0:\n",
        "    ch = logging.StreamHandler()\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
        "    ch.setFormatter(formatter)\n",
        "    ga_logger.addHandler(ch)\n",
        "\n",
        "\n",
        "rd_logger = RDLogger.logger()\n",
        "\n",
        "\n",
        "def make_mating_pool(population_mol: List, population_scores, offspring_size: int):\n",
        "    \"\"\"\n",
        "    Given a population of RDKit Mol and their scores, sample a list of the same size\n",
        "    with replacement using the population_scores as weights\n",
        "\n",
        "    Args:\n",
        "        population_mol: list of population\n",
        "        population_scores: list of un-normalised scores given by ScoringFunction\n",
        "        offspring_size: number of molecules to return\n",
        "\n",
        "    Returns: a list of RDKit Mol (probably not unique)\n",
        "\n",
        "    \"\"\"\n",
        "    # scores -> probs\n",
        "    sum_scores = sum(population_scores)\n",
        "    population_probs = [p / sum_scores for p in population_scores]\n",
        "    mating_pool = np.random.choice(population_mol, p=population_probs, size=offspring_size, replace=True)\n",
        "    return mating_pool\n",
        "\n",
        "\n",
        "def reproduce(mating_pool, mutation_rate, crossover_kwargs: dict = None):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        mating_pool: list of RDKit Mol\n",
        "        mutation_rate: rate of mutation\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # turn off rdkit logging\n",
        "    rd_logger.setLevel(RDLogger.CRITICAL)\n",
        "\n",
        "    parent_a = Chem.MolFromSmiles(random.choice(mating_pool))\n",
        "    parent_b = Chem.MolFromSmiles(random.choice(mating_pool))\n",
        "    if crossover_kwargs is None:\n",
        "        crossover_kwargs = dict()\n",
        "    new_child = crossover(parent_a, parent_b, **crossover_kwargs)\n",
        "    if new_child is not None:\n",
        "        new_child = mutate(new_child, mutation_rate)\n",
        "    return new_child\n",
        "\n",
        "\n",
        "def score_mol(mol, score_fn):\n",
        "    return score_fn(Chem.MolToSmiles(mol))\n",
        "\n",
        "\n",
        "def sanitize(population_mol):\n",
        "    new_population = []\n",
        "    smile_set = set()\n",
        "    for mol in population_mol:\n",
        "        if mol is not None:\n",
        "            try:\n",
        "                smile = Chem.MolToSmiles(mol)\n",
        "                if smile is not None and smile not in smile_set:\n",
        "                    smile_set.add(smile)\n",
        "                    new_population.append(mol)\n",
        "            except ValueError:\n",
        "                print(\"bad smiles\")\n",
        "    return new_population\n",
        "\n",
        "\n",
        "def sanitize_smiles(population_smiles):\n",
        "    new_population = []\n",
        "    smile_set = set()\n",
        "    for smiles in population_smiles:\n",
        "        if Chem.MolFromSmiles(smiles) is not None and smiles not in smile_set:\n",
        "            new_population.append(smiles)\n",
        "    return new_population\n",
        "\n",
        "\n",
        "def run_ga_maximization(\n",
        "    starting_population_smiles: list,\n",
        "    scoring_function: Union[callable, CachedFunction],\n",
        "    max_generations: int,\n",
        "    population_size: int,\n",
        "    offspring_size: int,\n",
        "    mutation_rate: float,\n",
        "    patience: int = None,\n",
        "    max_func_calls: int = None,\n",
        "    min_func_val: float = 0.0,\n",
        "    crossover_kwargs: dict = None,\n",
        "    y_transform: callable = None,  # only used if scoring function is not a cached function already\n",
        "    num_cpu: int = 1,\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs a genetic algorithm to MAXIMIZE a score function.\n",
        "\n",
        "    It does accurate budgeting by tracking which function calls have been made already.\n",
        "    Note that the function will always be called with canonical smiles.\n",
        "    \"\"\"\n",
        "    ga_logger.info(\"Starting GA maximization...\")\n",
        "    if crossover_kwargs is None:\n",
        "        crossover_kwargs = dict()\n",
        "\n",
        "    # Create the cached function\n",
        "    if not isinstance(scoring_function, CachedFunction):\n",
        "        scoring_function = CachedFunction(scoring_function, transform=y_transform)\n",
        "    start_cache = dict(scoring_function.cache)\n",
        "    start_cache_size = len(start_cache)\n",
        "    ga_logger.debug(f\"Starting cache made, has size {start_cache_size}\")\n",
        "\n",
        "    # Budget will just be measured by the cache size,\n",
        "    # But since the starting cache is free update the budget\n",
        "    # to account for this\n",
        "    if max_func_calls is not None:\n",
        "        max_func_calls += start_cache_size\n",
        "\n",
        "    # Init population and scores\n",
        "    population_smiles = list(set(starting_population_smiles))\n",
        "    num_start_eval = len(set(population_smiles) - set(start_cache.keys()))\n",
        "    ga_logger.debug(\n",
        "        \"Scoring initial population. \"\n",
        "        f\"{num_start_eval}/{len(population_smiles)} \"\n",
        "        f\"({num_start_eval/len(population_smiles)*100:.1f}%) \"\n",
        "        \"not in start cache and will need evaluation.\"\n",
        "    )\n",
        "    del num_start_eval  # not needed later\n",
        "    population_scores = scoring_function(population_smiles, batch=True)\n",
        "    queried_smiles = list(population_smiles)\n",
        "    ga_logger.debug(f\"Initial population scoring done. Pop size={len(population_smiles)}, Max={max(population_scores)}\")\n",
        "\n",
        "    # Run GA\n",
        "    early_stop = False\n",
        "    reached_budget = False\n",
        "    num_no_change_gen = 0\n",
        "    gen_info = []\n",
        "    with joblib.Parallel(n_jobs=num_cpu) as parallel:\n",
        "        for generation in range(max_generations):\n",
        "            ga_logger.info(f\"Start generation {generation}\")\n",
        "\n",
        "            # Make mating pool\n",
        "            # We use an adjusted score to weight each sample in the mating pool\n",
        "            bottom_score = np.min(population_scores)\n",
        "            if min_func_val is not None:\n",
        "                bottom_score = min(min_func_val, bottom_score)\n",
        "            mating_pool = make_mating_pool(\n",
        "                population_smiles,\n",
        "                np.asarray(population_scores) - bottom_score,\n",
        "                population_size,\n",
        "            )\n",
        "\n",
        "            # Create offspring in parallel to be more efficient\n",
        "            offspring_mol = parallel(\n",
        "                joblib.delayed(reproduce)(mating_pool, mutation_rate, **crossover_kwargs) for _ in range(offspring_size)\n",
        "            )\n",
        "            # offspring_mol = [\n",
        "            #     reproduce(mating_pool, mutation_rate, **crossover_kwargs)\n",
        "            #     for _ in range(offspring_size)\n",
        "            # ]\n",
        "            ga_logger.debug(f\"\\t{len(offspring_mol)} created\")\n",
        "\n",
        "            # Convert offspring to SMILES and add to new population\n",
        "            offspring_smiles = []\n",
        "            for mol in offspring_mol:\n",
        "                try:\n",
        "                    if mol is not None:\n",
        "                        offspring_smiles.append(Chem.MolToSmiles(mol))\n",
        "                except ValueError:\n",
        "                    pass\n",
        "            ga_logger.debug(f\"\\t{len(offspring_smiles)}/{len(offspring_mol)} converted to SMILES\")\n",
        "            population_and_offspring_smiles = population_smiles + offspring_smiles\n",
        "            population_and_offspring_smiles = list(set(population_and_offspring_smiles))\n",
        "            ga_logger.debug(f\"\\tPopulation sanitized, now contains {len(population_and_offspring_smiles)} members.\")\n",
        "\n",
        "            # Find out scores, but don't go over budget\n",
        "            old_scores = population_scores\n",
        "            population_smiles = []\n",
        "            planned_cached_smiles = set(scoring_function.cache.keys())  # make a copy to not go over budget\n",
        "            planned_cache_start_size = len(planned_cached_smiles)\n",
        "            for smiles in population_and_offspring_smiles:\n",
        "                if (\n",
        "                    max_func_calls is None\n",
        "                    or smiles in scoring_function.cache\n",
        "                    or len(planned_cached_smiles) < max_func_calls\n",
        "                ):\n",
        "                    population_smiles.append(smiles)\n",
        "                    planned_cached_smiles.add(smiles)\n",
        "            ga_logger.debug(\n",
        "                \"\\tDecided on which SMILES to evaluate. Plan to make \"\n",
        "                f\"{len(planned_cached_smiles) - planned_cache_start_size} new function calls.\"\n",
        "            )\n",
        "            ga_logger.debug(\"\\tStarting function calls...\")\n",
        "            population_scores = scoring_function(population_smiles, batch=True)\n",
        "            queried_smiles += population_smiles\n",
        "            ga_logger.debug(f\"\\tScoring done, best score now {max(population_scores)}.\")\n",
        "\n",
        "            # Trim population (take highest few values)\n",
        "            argsort = np.argsort(-np.asarray(population_scores))[:population_size]\n",
        "            population_smiles = [population_smiles[i] for i in argsort]\n",
        "            population_scores = [population_scores[i] for i in argsort]\n",
        "            ga_logger.debug(f\"\\tPopulation trimmed to size {len(population_smiles)}.\")\n",
        "\n",
        "            # Record results of generation\n",
        "            gen_stats_dict = dict(\n",
        "                max=np.max(population_scores),\n",
        "                avg=np.mean(population_scores),\n",
        "                median=np.median(population_scores),\n",
        "                min=np.min(population_scores),\n",
        "                std=np.std(population_scores),\n",
        "                size=len(population_scores),\n",
        "                num_func_eval=len(scoring_function.cache) - start_cache_size,\n",
        "            )\n",
        "            stats_str = \" \".join([\"\\tGen stats:\\n\"] + [f\"{k}={v}\" for k, v in gen_stats_dict.items()])\n",
        "            ga_logger.info(stats_str)\n",
        "            gen_info.append(dict(smiles=population_smiles, **gen_stats_dict))\n",
        "\n",
        "            # early stopping if population doesn't change\n",
        "            if len(population_scores) == len(old_scores) and np.allclose(population_scores, old_scores):\n",
        "                num_no_change_gen += 1\n",
        "                ga_logger.info(f\"\\tPopulation unchanged for {num_no_change_gen} generations\")\n",
        "                if patience is not None and num_no_change_gen > patience:\n",
        "                    ga_logger.info(f\"\\tThis exceeds patience of {patience}. Terminating GA.\")\n",
        "                    early_stop = True\n",
        "                    break\n",
        "            else:\n",
        "                num_no_change_gen = 0\n",
        "\n",
        "            # early stopping if budget is reached\n",
        "            if max_func_calls is not None and len(scoring_function.cache) >= max_func_calls:\n",
        "                ga_logger.info(f\"\\tBudget of {max_func_calls - start_cache_size} has been reached. Terminating...\")\n",
        "                reached_budget = True\n",
        "                break\n",
        "\n",
        "    # Before returning, filter duplicates from the queried SMILES\n",
        "    queried_smiles_set = set()\n",
        "    new_queried_smiles = list()\n",
        "    for s in queried_smiles:\n",
        "        if s not in queried_smiles_set:\n",
        "            queried_smiles_set.add(s)\n",
        "            new_queried_smiles.append(s)\n",
        "    queried_smiles = new_queried_smiles\n",
        "\n",
        "    # Return values\n",
        "    ga_logger.info(\"End of GA. Returning results.\")\n",
        "    return (\n",
        "        queried_smiles,  # all smiles queried in order, *without* duplicates\n",
        "        scoring_function.cache,  # holds function values\n",
        "        (gen_info, early_stop, reached_budget),  # GA logs\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqsd2TZCk04C"
      },
      "outputs": [],
      "source": [
        "\"\"\" Contains for for Gaussian process Bayesian optimization \"\"\"\n",
        "\n",
        "import heapq\n",
        "import logging\n",
        "import pprint\n",
        "import random\n",
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Logger with standard handler\n",
        "logger = logging.getLogger(\"gp_bo\")\n",
        "if len(logger.handlers) == 0:\n",
        "    ch = logging.StreamHandler()\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(ch)\n",
        "\n",
        "\n",
        "# Optimize acquisition function with genetic algorithm\n",
        "def maximize_acquisition_func_ga(\n",
        "    gp_model: TanimotoGP,\n",
        "    acq_func_np: callable,\n",
        "    starting_smiles: list,\n",
        "    smiles_to_np_fingerprint: callable,\n",
        "    **ga_kwargs,\n",
        "):\n",
        "    # Construct acquisition function for GA\n",
        "    def _acq_func_smiles(smiles_list):\n",
        "        fp_array = np.stack(list(map(smiles_to_np_fingerprint, smiles_list)))\n",
        "        fp_array = fp_array.astype(np.float32)\n",
        "        mu_pred, var_pred = batch_predict_mu_var_numpy(gp_model, torch.as_tensor(fp_array))\n",
        "        acq_vals = acq_func_np(mu_pred, var_pred)\n",
        "        return list(map(float, acq_vals))\n",
        "\n",
        "    cached_acq_function = CachedBatchFunction(_acq_func_smiles)\n",
        "\n",
        "    # Run GA\n",
        "    _, smiles_2_acq_dict, _ = run_ga_maximization(\n",
        "        starting_population_smiles=list(starting_smiles),\n",
        "        scoring_function=cached_acq_function,\n",
        "        **ga_kwargs,\n",
        "    )\n",
        "\n",
        "    # Sort and return results (highest acq func first)\n",
        "    sm_ac_list = list(smiles_2_acq_dict.items())\n",
        "    sm_ac_list.sort(reverse=True, key=lambda t: t[1])\n",
        "    smiles_out = [s for s, v in sm_ac_list]\n",
        "    acq_out = [v for s, v in sm_ac_list]\n",
        "    return smiles_out, acq_out\n",
        "\n",
        "\n",
        "# Whole GP BO loop\n",
        "def gp_bo_loop(\n",
        "    gp_model: TanimotoGP,\n",
        "    scoring_function: Union[callable, CachedFunction],\n",
        "    smiles_to_np_fingerprint: callable,\n",
        "    acq_func_of_time: callable,\n",
        "    max_bo_iter: int,\n",
        "    bo_batch_size: int = 1,\n",
        "    y_transform: callable = None,\n",
        "    gp_train_smiles: list = None,\n",
        "    smiles_pool: list = None,\n",
        "    max_func_calls: int = None,\n",
        "    ga_pool_num_best: int = 250,\n",
        "    ga_pool_num_carryover: int = 250,  # number of SMILES with high acq funcs to carry over from last time\n",
        "    max_ga_start_population_size: int = 1000,\n",
        "    ga_population_size: int = 500,\n",
        "    ga_max_generations: int = 25,\n",
        "    ga_offspring_size: int = 1000,\n",
        "    ga_mutation_rate: float = 1e-2,\n",
        "    ga_num_cpu: int = 1,\n",
        "    refit_gp_func: callable = None,\n",
        "    n_top_log: int = 10,  # When I log \"topN\" of something, what should N be?\n",
        "    log_ga_smiles: bool = False,  # whether to log all SMILES evaluated with GA.\n",
        "):\n",
        "    logger.info(\"Starting GP BO\")\n",
        "\n",
        "    # Create the cached function\n",
        "    if not isinstance(scoring_function, CachedFunction):\n",
        "        scoring_function = CachedFunction(scoring_function, transform=y_transform)\n",
        "    start_cache = dict(scoring_function.cache)\n",
        "    start_cache_size = len(start_cache)\n",
        "    logger.debug(f\"Starting cache made, has size {start_cache_size}\")\n",
        "    logger.info(\n",
        "        f\"Top {n_top_log} known starting scores:\\n\"\n",
        "        + \", \".join(\n",
        "            f\"#{i+1}={v:.3f}\"\n",
        "            for i, v in enumerate(heapq.nlargest(n_top_log, scoring_function(list(start_cache.keys()), batch=True)))\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Set up which SMILES the GP should be trained on\n",
        "    # If not given, it is assumed that the GP is trained on all known smiles\n",
        "    if gp_train_smiles is None:\n",
        "        logger.debug(\n",
        "            \"No GP training SMILES given. \"\n",
        "            f\"Will default to training on the {start_cache_size} SMILES with known scores.\"\n",
        "        )\n",
        "        gp_train_smiles_set = set(start_cache.keys())\n",
        "    else:\n",
        "        gp_train_smiles_set = set(gp_train_smiles)\n",
        "    del gp_train_smiles  # should refer to new variables later on; don't want to use by mistake\n",
        "\n",
        "    # Keep a pool of all SMILES encountered (used for seeding GA)\n",
        "    if smiles_pool is None:\n",
        "        smiles_pool = set()\n",
        "    else:\n",
        "        smiles_pool = set(smiles_pool)\n",
        "    smiles_pool.update(start_cache.keys())\n",
        "    smiles_pool.update(gp_train_smiles_set)\n",
        "    logger.debug(f\"SMILES pool created, size={len(smiles_pool)}\")\n",
        "    assert (\n",
        "        len(smiles_pool) > 0\n",
        "    ), \"No SMILES were provided to the algorithm as training data, known scores, or a SMILES pool.\"\n",
        "\n",
        "    # Handle edge case of no training data\n",
        "    if len(gp_train_smiles_set) == 0:\n",
        "        logger.warning(\n",
        "            \"No SMILES were provided to train GP. A random one will be chosen from the pool to start training.\"\n",
        "        )\n",
        "        random_smiles = random.choice(list(smiles_pool))\n",
        "        logger.debug(f\"The following SMILES was chosen:\\n\\t{random_smiles}\")\n",
        "        gp_train_smiles_set.add(random_smiles)\n",
        "        del random_smiles\n",
        "    if len(gp_train_smiles_set) > 0:\n",
        "        logger.debug(f\"Plan to condition GP on {len(gp_train_smiles_set)} training points.\")\n",
        "\n",
        "    # Evaluate scores of training data (ideally should all be known)\n",
        "    num_train_data_not_known = len(gp_train_smiles_set - set(start_cache.keys()))\n",
        "    if num_train_data_not_known > 0:\n",
        "        logger.warning(\n",
        "            f\"Need to evaluate {num_train_data_not_known} unknown GP training points.\"\n",
        "            \" Probably the training points should have known scores which should be provided.\"\n",
        "        )\n",
        "    logger.debug(\"Scoring training points.\")\n",
        "    gp_train_smiles_list = list(gp_train_smiles_set)\n",
        "    gp_train_smiles_scores = scoring_function(gp_train_smiles_list, batch=True)\n",
        "    logger.debug(\"Scoring of training points done.\")\n",
        "\n",
        "    # Store GP training data\n",
        "    x_train_np = np.stack(list(map(smiles_to_np_fingerprint, gp_train_smiles_list))).astype(np.float32)\n",
        "    y_train_np = np.array(gp_train_smiles_scores).astype(np.float32)\n",
        "    gp_model.set_train_data(\n",
        "        inputs=torch.as_tensor(x_train_np),\n",
        "        targets=torch.as_tensor(y_train_np),\n",
        "        strict=False,\n",
        "    )\n",
        "    logger.debug(\"Created initial GP training data\")\n",
        "\n",
        "    # Possibly re-fit GP hyperparameters\n",
        "    if refit_gp_func is not None:\n",
        "        logger.info(\"Initial fitting of GP hyperparameters\")\n",
        "        raise NotImplementedError  # call refit_gp_func(0)\n",
        "\n",
        "    # Do the BO loop\n",
        "    carryover_smiles_pool = set()\n",
        "    bo_query_res = list()\n",
        "    bo_state_dict = dict(\n",
        "        gp_model=gp_model,\n",
        "        gp_train_smiles_list=gp_train_smiles_list,\n",
        "        bo_query_res=bo_query_res,\n",
        "    )\n",
        "    for bo_iter in range(1, max_bo_iter + 1):\n",
        "        logger.info(f\"Start iter {bo_iter}\")\n",
        "\n",
        "        # Make starting population for GA from a combination of\n",
        "        #     1) best `ga_pool_num_best` known scores\n",
        "        #     2) Up to `ga_pool_num_carryover` promising SMILES from last iteration\n",
        "        #     3) Random smiles from `smiles_pool` to pad the pool\n",
        "        top_smiles_at_bo_iter_start = [\n",
        "            s\n",
        "            for _, s in heapq.nlargest(\n",
        "                ga_pool_num_best,\n",
        "                [(scoring_function(smiles), smiles) for smiles in scoring_function.cache.keys()],\n",
        "            )\n",
        "        ]\n",
        "        ga_start_smiles = set(top_smiles_at_bo_iter_start)  # start with best\n",
        "        ga_start_smiles.update(carryover_smiles_pool)  # add carryover\n",
        "        if len(ga_start_smiles) < max_ga_start_population_size:\n",
        "            samples_from_pool = random.sample(smiles_pool, min(len(smiles_pool), max_ga_start_population_size))\n",
        "\n",
        "            # Pad with random SMILES until full\n",
        "            for s in samples_from_pool:\n",
        "                ga_start_smiles.add(s)\n",
        "                if len(ga_start_smiles) >= max_ga_start_population_size:\n",
        "                    break\n",
        "            del samples_from_pool\n",
        "\n",
        "        # Current acquisition function\n",
        "        curr_acq_func = acq_func_of_time(bo_iter, bo_state_dict)\n",
        "\n",
        "        # Optimize acquisition function\n",
        "        logger.debug(f\"Maximizing acqusition function with {len(ga_start_smiles)} starting SMILES.\")\n",
        "        acq_smiles, acq_vals = maximize_acquisition_func_ga(\n",
        "            gp_model=gp_model,\n",
        "            acq_func_np=curr_acq_func,\n",
        "            starting_smiles=list(ga_start_smiles),\n",
        "            smiles_to_np_fingerprint=smiles_to_np_fingerprint,\n",
        "            max_generations=ga_max_generations,\n",
        "            population_size=ga_population_size,\n",
        "            offspring_size=ga_offspring_size,\n",
        "            mutation_rate=ga_mutation_rate,\n",
        "            num_cpu=ga_num_cpu,\n",
        "        )\n",
        "        logger.debug(f\"Acquisition function optimized, {len(acq_smiles)} evaluated.\")\n",
        "        _n_top = max(n_top_log, bo_batch_size + 3)\n",
        "        logger.debug(f\"Top {_n_top} acquisition function values: \" + \", \".join([f\"{v:.2f}\" for v in acq_vals[:_n_top]]))\n",
        "        del _n_top\n",
        "\n",
        "        # Now that new SMILES were generated, add them to the pool\n",
        "        _start_size = len(smiles_pool)\n",
        "        smiles_pool.update(acq_smiles)\n",
        "        _end_size = len(smiles_pool)\n",
        "        logger.debug(f\"{_end_size - _start_size} smiles added to pool \" f\"(size went from {_start_size} to {_end_size}\")\n",
        "        del _start_size, _end_size\n",
        "\n",
        "        # Greedily choose SMILES to be in the BO batch\n",
        "        smiles_batch = []\n",
        "        smiles_batch_acq = []\n",
        "        for candidate_smiles, acq in zip(acq_smiles, acq_vals):\n",
        "            if candidate_smiles not in gp_train_smiles_set:\n",
        "                smiles_batch.append(candidate_smiles)\n",
        "                smiles_batch_acq.append(acq)\n",
        "            if len(smiles_batch) >= bo_batch_size:\n",
        "                break\n",
        "        del candidate_smiles, acq\n",
        "        logger.debug(f\"Batch created, size {len(smiles_batch)}/{bo_batch_size}\")\n",
        "        assert len(smiles_batch) > 0, \"Empty batch, shouldn't happen. Must be problem with GA.\"\n",
        "        smiles_batch_np = np.stack(list(map(smiles_to_np_fingerprint, smiles_batch))).astype(x_train_np.dtype)\n",
        "\n",
        "        # Get predictions about SMILES batch before training on it\n",
        "        smiles_batch_mu_pre, smiles_batch_var_pre = batch_predict_mu_var_numpy(\n",
        "            gp_model, torch.as_tensor(smiles_batch_np)\n",
        "        )\n",
        "        logger.debug(\"Made mean/var predictions for new SMILES batch\")\n",
        "\n",
        "        # Score these SMILES\n",
        "        logger.debug(f\"Evaluating scoring function on SMILES batch of size {len(smiles_batch)}.\")\n",
        "        smiles_batch_scores = scoring_function(smiles_batch, batch=True)\n",
        "        logger.debug(\"Scoring complete.\")\n",
        "\n",
        "        # Add new points to GP training data\n",
        "        gp_train_smiles_list += smiles_batch\n",
        "        gp_train_smiles_set.update(gp_train_smiles_list)\n",
        "        x_train_np = np.concatenate([x_train_np, smiles_batch_np], axis=0)\n",
        "        y_train_np = np.concatenate(\n",
        "            [y_train_np, np.asarray(smiles_batch_scores, dtype=y_train_np.dtype)],\n",
        "            axis=0,\n",
        "        )\n",
        "        gp_model.set_train_data(\n",
        "            inputs=torch.as_tensor(x_train_np),\n",
        "            targets=torch.as_tensor(y_train_np),\n",
        "            strict=False,\n",
        "        )\n",
        "        logger.debug(f\"GP training data reset, now of size {len(x_train_np)}\")\n",
        "\n",
        "        # Add SMILES with high acquisition function values to the priority pool,\n",
        "        # Since maybe they will have high acquisition function values next time\n",
        "        carryover_smiles_pool = set()\n",
        "        for s in acq_smiles:\n",
        "            if len(carryover_smiles_pool) < ga_pool_num_carryover and s not in gp_train_smiles_set:\n",
        "                carryover_smiles_pool.add(s)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # Get predictions about SMILES batch AFTER training on it\n",
        "        smiles_batch_mu_post1, smiles_batch_var_post1 = batch_predict_mu_var_numpy(\n",
        "            gp_model, torch.as_tensor(smiles_batch_np)\n",
        "        )\n",
        "\n",
        "        # Potentially refit GP hyperparameters\n",
        "        if refit_gp_func is not None:\n",
        "            logger.info(\"Re-fitting GP hyperparameters\")\n",
        "            raise NotImplementedError  # call refit_gp_func(bo_iter), and make more predictions!\n",
        "\n",
        "        # Assemble full batch results\n",
        "        batch_results = []\n",
        "        for i, s in enumerate(smiles_batch):\n",
        "            transformed_score = scoring_function(s, batch=False)\n",
        "            pred_dict = dict(\n",
        "                mu=float(smiles_batch_mu_pre[i]),\n",
        "                std=float(np.sqrt(smiles_batch_var_pre[i])),\n",
        "                acq=smiles_batch_acq[i],\n",
        "            )\n",
        "            pred_dict[\"pred_error_in_stds\"] = (pred_dict[\"mu\"] - transformed_score) / pred_dict[\"std\"]\n",
        "            pred_dict_post1 = dict(\n",
        "                mu=float(smiles_batch_mu_post1[i]),\n",
        "                std=float(np.sqrt(smiles_batch_var_post1[i])),\n",
        "            )\n",
        "            res = dict(\n",
        "                bo_iter=bo_iter,\n",
        "                smiles=s,\n",
        "                raw_score=scoring_function.cache[s],\n",
        "                transformed_score=transformed_score,\n",
        "                predictions=pred_dict,\n",
        "                predictions_after_fit=pred_dict_post1,\n",
        "            )\n",
        "            batch_results.append(res)\n",
        "\n",
        "            del pred_dict, pred_dict_post1, res, transformed_score\n",
        "        bo_query_res += batch_results\n",
        "        logger.debug(\"Full batch results:\\n\" + pprint.pformat(batch_results))\n",
        "\n",
        "        # Potentially add GA info to batch\n",
        "        if log_ga_smiles:\n",
        "            batch_results[0][\"ga_info\"] = dict(\n",
        "                ga_start_smiles=ga_start_smiles,\n",
        "                ga_eval_smiles=acq_smiles,\n",
        "            )\n",
        "\n",
        "        # Log batch information\n",
        "        bo_iter_status_update = f\"End of iter {bo_iter}. Status update:\"\n",
        "        # bo_iter_status_update += \"\\n\\tBatch scores (raw): \"\n",
        "        # bo_iter_status_update += \", \".join([str(r[\"raw_score\"]) for r in batch_results])\n",
        "        bo_iter_status_update += \"\\n\\tBatch scores (transformed): \"\n",
        "        bo_iter_status_update += \", \".join([str(r[\"transformed_score\"]) for r in batch_results])\n",
        "        bo_iter_status_update += \"\\n\\tBatch acquisition function values: \"\n",
        "        bo_iter_status_update += \", \".join(f\"{a:.2f}\" for a in smiles_batch_acq)\n",
        "        bo_iter_status_update += \"\\n\\tAcquisition function values of top known smiles : \"\n",
        "        _acq_val_dict = dict(zip(acq_smiles, acq_vals))\n",
        "        bo_iter_status_update += \", \".join(f\"{_acq_val_dict[s]:.2f}\" for s in top_smiles_at_bo_iter_start[:n_top_log])\n",
        "        del _acq_val_dict\n",
        "\n",
        "        # Overall progress towards optimizing function\n",
        "        new_bo_smiles = [r[\"smiles\"] for r in bo_query_res if r[\"smiles\"] not in start_cache]\n",
        "        new_bo_smiles = list(set(new_bo_smiles))\n",
        "        bo_iter_status_update += \"\\n\\tTop new scores so far: \"\n",
        "        bo_iter_status_update += \", \".join(\n",
        "            f\"#{i+1}={v:.3f}\"\n",
        "            for i, v in enumerate(heapq.nlargest(n_top_log, [scoring_function(s) for s in new_bo_smiles]))\n",
        "        )\n",
        "        func_evals_so_far = len(scoring_function.cache) - start_cache_size\n",
        "        bo_iter_status_update += f\"\\n\\tFunction calls so far: {func_evals_so_far}\"\n",
        "        logger.info(bo_iter_status_update)\n",
        "\n",
        "        # Delete variables that shouldn't persist for multiple runs of loop\n",
        "        # (not strictly necessary but reduces probability of bug)\n",
        "        del ga_start_smiles\n",
        "        del smiles_batch, acq_smiles, acq_vals, smiles_batch_scores, smiles_batch_np\n",
        "        del (\n",
        "            smiles_batch_mu_pre,\n",
        "            smiles_batch_mu_post1,\n",
        "            smiles_batch_var_pre,\n",
        "            smiles_batch_var_post1,\n",
        "        )\n",
        "        del batch_results, bo_iter_status_update\n",
        "\n",
        "        # Potentially do early stopping if too many function calls were made\n",
        "        if max_func_calls is not None and func_evals_so_far >= max_func_calls:\n",
        "            logger.info(\"Maximum number of function evaluations reached. STOPPING.\")\n",
        "            break\n",
        "        del func_evals_so_far\n",
        "\n",
        "    logger.info(\"End of BO loop.\")\n",
        "    return (bo_query_res, scoring_function.cache)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFC3BuCJZ2DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WktCo8nlILDC",
        "outputId": "05b606a4-4de7-4ef2-b19f-9efd3a9de00a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'dockstring'...\n",
            "remote: Enumerating objects: 2269, done.\u001b[K\n",
            "remote: Counting objects: 100% (708/708), done.\u001b[K\n",
            "remote: Compressing objects: 100% (343/343), done.\u001b[K\n",
            "remote: Total 2269 (delta 402), reused 607 (delta 357), pack-reused 1561\u001b[K\n",
            "Receiving objects: 100% (2269/2269), 16.37 MiB | 12.91 MiB/s, done.\n",
            "Resolving deltas: 100% (1001/1001), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dockstring/dockstring\n",
        "!git clone https://github.com/dockstring/experiments-from-paper"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}